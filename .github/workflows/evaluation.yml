name: Evaluation
on:
  push:
    branches:
      - "main"
  pull_request:

permissions:
  contents: write

jobs:
  eval-build:
    name: Build Lean-MLIR
    runs-on: ubuntu-latest
    steps:
      - name: Checkout ðŸ›Žï¸
        uses: actions/checkout@v3

      - name: Cache `.lake` and `.elan` folders
        id: cache-lake
        uses: actions/cache@v4
        with:
          lookup-only: true
          path: |
            ~/.elan
            .lake/packages
            .lake/build
            TacBench/.lake
          key: ${{ runner.os }}-lake-packages-${{ hashFiles('lake-manifest.json') }}

      - name: Install Elan & Lean
        if: steps.cache-lake.outputs.cache-hit != 'true'
        run: |
          set -o pipefail
          curl https://raw.githubusercontent.com/leanprover/elan/master/elan-init.sh -sSf | sh -s -- --default-toolchain none -y
          ~/.elan/bin/lean --version
          echo "$HOME/.elan/bin" >> $GITHUB_PATH

      - name: Build Lean-MLIR & Mathlib
        # We build Mathlib from scratch as this reduces the build artifacts that are stored in the cache. Previous experiments
        # led to savings from avoiding a full Mathlib cache in the order of a couple hundred MBs.
        if: steps.cache-lake.outputs.cache-hit != 'true'
        run: |
          lake -R build

  extract-goals:
    name: Check that extract-goals remains unchanged
    runs-on: ubuntu-latest
    needs: eval-build
    permissions:
      pull-requests: write
    steps:
      - name: Checkout ðŸ›Žï¸
        uses: actions/checkout@v3

      - name: Load Lean and Lean-MLIR from Cache
        id: cache-lake
        uses: actions/cache/restore@v4
        with:
          path: |
            ~/.elan
            .lake/packages
            .lake/build
            TacBench/.lake
          key: ${{ runner.os }}-lake-packages-${{ hashFiles('lake-manifest.json') }}

      - name: Add Lean to PATH
        run: |
          echo "$HOME/.elan/bin" >> $GITHUB_PATH

      - name: Build Lean-MLIR (should be all in cache)
        run: |
          lake -R build

      - name: Install Python Modules
        run: |
          sudo apt-get update
          sudo apt-get install -y python3-matplotlib python3-pandas python3-num2words python3-psutil ripgrep

      - name: Ensure InstCombine goals are up-to-date
        continue-on-error: true
        run: |
          bash SSA/Projects/InstCombine/scripts/test-extract-goals.sh

  evaluation:
    name: Evaluation of Tactics & Decision Procedures
    runs-on: ubuntu-latest
    needs: eval-build
    permissions:
      pull-requests: write
    steps:
      - name: Checkout ðŸ›Žï¸
        uses: actions/checkout@v3

      - name: Load Lean and Lean-MLIR from Cache
        id: cache-lake
        uses: actions/cache/restore@v4
        with:
          path: |
            ~/.elan
            .lake/packages
            .lake/build
            TacBench/.lake
          key: ${{ runner.os }}-lake-packages-${{ hashFiles('lake-manifest.json') }}

      - name: Add Lean to PATH
        run: |
          echo "$HOME/.elan/bin" >> $GITHUB_PATH

      - name: Build Lean-MLIR (should be all in cache)
        run: |
          lake -R build

      - name: Install Python Modules
        run: |
          sudo apt-get update
          sudo apt-get install -y python3-matplotlib python3-pandas python3-num2words python3-psutil ripgrep

      - name: Run LLVM
        continue-on-error: true
        run: |
          cd bv-evaluation
          python3 ./compare.py instcombine -j48 \
            || echo "LEANMLIR_STATUS=fail" >> $GITHUB_ENV

      - name: Collect data LLVM
        continue-on-error: true
        run: |
          cd bv-evaluation
          (python3 ./collect.py instcombine | tee llvm-stats) \
            || echo "LEANMLIR_STATUS=fail" >> $GITHUB_ENV

      - uses: actions/github-script@v6
        if: env.LEANMLIR_STATUS != 'fail' && github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs')
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: fs.readFileSync('bv-evaluation/llvm-stats', 'utf8')
            })

      - name: Upload LLVM artifact
        uses: actions/upload-artifact@v4
        with:
          name: LLVM evaluation
          path: bv-evaluation/results

      - name: Run Alive Symbolic
        run: |
          (cd bv-evaluation; python3 ./compare-leansat-vs-bitwuzla-alive-sym.py -j48)

      - name: Compare Alive All
        continue-on-error: true
        run: |
          (cd bv-evaluation; python3 ./collect-data-alive.py > /dev/null)

      - name: Run HDel Symbolic
        run: |
          (cd bv-evaluation; python3 ./compare-leansat-vs-bitwuzla-hdel-sym.py -j48)

      - name: Compare HDEl Symbolic
        continue-on-error: true
        run: |
          (cd bv-evaluation; python3 ./collect-data-hdel-symbolic.py > /dev/null)

      - name: Run HDel
        run: |
          (cd bv-evaluation; python3 ./compare.py hackersdelight -j48)

      - name: Collect data HDEl
        continue-on-error: true
        run: |
          (cd bv-evaluation; python3 ./collect.py hackersdelight > /dev/null)

#    - name: Run LLVM Symbolic
#        continue-on-error: true
#        run: |
#          (cd bv-evaluation; python3 ./compare-leansat-vs-bitwuzla-llvm-sym.py -j48)

#      - name: Compare LLVM Symbolic
#        continue-on-error: true
#        run: |
#          (cd bv-evaluation; python3 ./collect-data-llvm-symbolic.py > /dev/null)

      - name: Collect Stats
        continue-on-error: true
        run: |
          (cd bv-evaluation/tools; python3 ./collect-stats-bv-decide.py)
