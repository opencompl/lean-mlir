-- AUTOGENERATED Lean file

import SSA.Projects.LLVMRiscV.PeepholeRefine
import SSA.Projects.LLVMRiscV.Simpproc
import SSA.Projects.RISCV64.Tactic.SimpRiscV
import SSA.Projects.LLVMRiscV.Pipeline.mkRewrite

open LLVMRiscV


/-! ### sub_to_add -/

/-- 
Test the rewrite:
  (sub x, C) → (add x, -C)
-/
def sub_to_add_neg50 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg49 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg48 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg47 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg46 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg45 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg44 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg43 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg42 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg41 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg40 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg39 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg38 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg37 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg36 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg35 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg34 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg33 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg32 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg31 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg30 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg29 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg28 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg27 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg26 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg25 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg24 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg23 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg22 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg21 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg20 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg19 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg18 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg17 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg16 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg15 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg14 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg13 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg12 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg11 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg10 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg9 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg8 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg7 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg6 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg5 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg4 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg3 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_3 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_4 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_5 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_6 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_7 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_8 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_9 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_10 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_11 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_12 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_13 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_14 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_15 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_16 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_17 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_18 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_19 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_20 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_21 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_22 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_23 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_24 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_25 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_26 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_27 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_28 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_29 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_30 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_31 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_32 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_33 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_34 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_35 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_36 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_37 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_38 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_39 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_40 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_41 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_42 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_43 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_44 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_45 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_46 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_47 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_48 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_49 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_50 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add : List (Σ Γ, LLVMPeepholeRewriteRefine 64 Γ) :=
  [
    ⟨_, sub_to_add_neg50⟩,
    ⟨_, sub_to_add_neg49⟩,
    ⟨_, sub_to_add_neg48⟩,
    ⟨_, sub_to_add_neg47⟩,
    ⟨_, sub_to_add_neg46⟩,
    ⟨_, sub_to_add_neg45⟩,
    ⟨_, sub_to_add_neg44⟩,
    ⟨_, sub_to_add_neg43⟩,
    ⟨_, sub_to_add_neg42⟩,
    ⟨_, sub_to_add_neg41⟩,
    ⟨_, sub_to_add_neg40⟩,
    ⟨_, sub_to_add_neg39⟩,
    ⟨_, sub_to_add_neg38⟩,
    ⟨_, sub_to_add_neg37⟩,
    ⟨_, sub_to_add_neg36⟩,
    ⟨_, sub_to_add_neg35⟩,
    ⟨_, sub_to_add_neg34⟩,
    ⟨_, sub_to_add_neg33⟩,
    ⟨_, sub_to_add_neg32⟩,
    ⟨_, sub_to_add_neg31⟩,
    ⟨_, sub_to_add_neg30⟩,
    ⟨_, sub_to_add_neg29⟩,
    ⟨_, sub_to_add_neg28⟩,
    ⟨_, sub_to_add_neg27⟩,
    ⟨_, sub_to_add_neg26⟩,
    ⟨_, sub_to_add_neg25⟩,
    ⟨_, sub_to_add_neg24⟩,
    ⟨_, sub_to_add_neg23⟩,
    ⟨_, sub_to_add_neg22⟩,
    ⟨_, sub_to_add_neg21⟩,
    ⟨_, sub_to_add_neg20⟩,
    ⟨_, sub_to_add_neg19⟩,
    ⟨_, sub_to_add_neg18⟩,
    ⟨_, sub_to_add_neg17⟩,
    ⟨_, sub_to_add_neg16⟩,
    ⟨_, sub_to_add_neg15⟩,
    ⟨_, sub_to_add_neg14⟩,
    ⟨_, sub_to_add_neg13⟩,
    ⟨_, sub_to_add_neg12⟩,
    ⟨_, sub_to_add_neg11⟩,
    ⟨_, sub_to_add_neg10⟩,
    ⟨_, sub_to_add_neg9⟩,
    ⟨_, sub_to_add_neg8⟩,
    ⟨_, sub_to_add_neg7⟩,
    ⟨_, sub_to_add_neg6⟩,
    ⟨_, sub_to_add_neg5⟩,
    ⟨_, sub_to_add_neg4⟩,
    ⟨_, sub_to_add_neg3⟩,
    ⟨_, sub_to_add_neg2⟩,
    ⟨_, sub_to_add_neg1⟩,
    ⟨_, sub_to_add_0⟩,
    ⟨_, sub_to_add_1⟩,
    ⟨_, sub_to_add_2⟩,
    ⟨_, sub_to_add_3⟩,
    ⟨_, sub_to_add_4⟩,
    ⟨_, sub_to_add_5⟩,
    ⟨_, sub_to_add_6⟩,
    ⟨_, sub_to_add_7⟩,
    ⟨_, sub_to_add_8⟩,
    ⟨_, sub_to_add_9⟩,
    ⟨_, sub_to_add_10⟩,
    ⟨_, sub_to_add_11⟩,
    ⟨_, sub_to_add_12⟩,
    ⟨_, sub_to_add_13⟩,
    ⟨_, sub_to_add_14⟩,
    ⟨_, sub_to_add_15⟩,
    ⟨_, sub_to_add_16⟩,
    ⟨_, sub_to_add_17⟩,
    ⟨_, sub_to_add_18⟩,
    ⟨_, sub_to_add_19⟩,
    ⟨_, sub_to_add_20⟩,
    ⟨_, sub_to_add_21⟩,
    ⟨_, sub_to_add_22⟩,
    ⟨_, sub_to_add_23⟩,
    ⟨_, sub_to_add_24⟩,
    ⟨_, sub_to_add_25⟩,
    ⟨_, sub_to_add_26⟩,
    ⟨_, sub_to_add_27⟩,
    ⟨_, sub_to_add_28⟩,
    ⟨_, sub_to_add_29⟩,
    ⟨_, sub_to_add_30⟩,
    ⟨_, sub_to_add_31⟩,
    ⟨_, sub_to_add_32⟩,
    ⟨_, sub_to_add_33⟩,
    ⟨_, sub_to_add_34⟩,
    ⟨_, sub_to_add_35⟩,
    ⟨_, sub_to_add_36⟩,
    ⟨_, sub_to_add_37⟩,
    ⟨_, sub_to_add_38⟩,
    ⟨_, sub_to_add_39⟩,
    ⟨_, sub_to_add_40⟩,
    ⟨_, sub_to_add_41⟩,
    ⟨_, sub_to_add_42⟩,
    ⟨_, sub_to_add_43⟩,
    ⟨_, sub_to_add_44⟩,
    ⟨_, sub_to_add_45⟩,
    ⟨_, sub_to_add_46⟩,
    ⟨_, sub_to_add_47⟩,
    ⟨_, sub_to_add_48⟩,
    ⟨_, sub_to_add_49⟩,
    ⟨_, sub_to_add_50⟩
  ]



/-! ### mul_to_shl -/

/--
Test the rewrite:
    (x * 2^n) → (x << n)
-/

def mul_to_shl_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (0) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_4 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_8 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (3) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_16 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (16) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_32 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (32) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (5) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl : List (Σ Γ, LLVMPeepholeRewriteRefine 64 Γ) :=
  [
    ⟨_, mul_to_shl_1⟩,
    ⟨_, mul_to_shl_2⟩,
    ⟨_, mul_to_shl_4⟩,
    ⟨_, mul_to_shl_8⟩,
    ⟨_, mul_to_shl_16⟩,
    ⟨_, mul_to_shl_32⟩
  ]



/-! ### urem_pow2_to_mask -/

/--
Test the rewrite:
    urem(x, 2^n) -> x & (2^n - 1)
-/

def urem_pow2_to_mask_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (0) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_4 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (3) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_8 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (7) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_16 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (16) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (15) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_32 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (32) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (31) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask : List (Σ Γ, LLVMPeepholeRewriteRefine 64 Γ) :=
  [
    ⟨_, urem_pow2_to_mask_1⟩,
    ⟨_, urem_pow2_to_mask_2⟩,
    ⟨_, urem_pow2_to_mask_4⟩,
    ⟨_, urem_pow2_to_mask_8⟩,
    ⟨_, urem_pow2_to_mask_16⟩,
    ⟨_, urem_pow2_to_mask_32⟩
  ]
