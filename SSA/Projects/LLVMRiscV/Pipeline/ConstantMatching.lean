-- AUTOGENERATED Lean file

import SSA.Projects.LLVMRiscV.PeepholeRefine
import SSA.Projects.LLVMRiscV.Simpproc
import SSA.Projects.RISCV64.Tactic.SimpRiscV
import SSA.Projects.LLVMRiscV.Pipeline.mkRewrite

open LLVMRiscV


/-! ### sub_to_add -/

/--
Test the rewrite:
  (sub x, C) â†’ (add x, -C)
-/
def sub_to_add_neg50 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg49 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg48 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg47 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg46 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg45 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg44 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg43 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg42 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg41 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg40 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg39 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg38 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg37 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg36 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg35 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg34 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg33 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg32 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg31 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg30 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg29 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg28 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg27 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg26 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg25 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg24 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg23 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg22 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg21 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg20 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg19 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg18 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg17 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg16 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg15 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg14 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg13 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg12 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg11 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg10 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg9 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg8 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg7 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg6 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg5 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg4 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg3 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_3 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_4 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_5 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_6 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_7 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_8 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_9 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_10 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_11 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_12 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_13 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_14 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_15 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_16 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_17 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_18 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_19 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_20 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_21 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_22 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_23 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_24 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_25 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_26 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_27 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_28 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_29 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_30 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_31 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_32 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_33 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_34 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_35 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_36 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_37 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_38 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_39 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_40 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_41 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_42 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_43 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_44 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_45 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_46 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_47 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_48 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_49 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_50 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add : List (Î£ Î“, LLVMPeepholeRewriteRefine 64 Î“) :=
  [
    âŸ¨_, sub_to_add_neg50âŸ©,
    âŸ¨_, sub_to_add_neg49âŸ©,
    âŸ¨_, sub_to_add_neg48âŸ©,
    âŸ¨_, sub_to_add_neg47âŸ©,
    âŸ¨_, sub_to_add_neg46âŸ©,
    âŸ¨_, sub_to_add_neg45âŸ©,
    âŸ¨_, sub_to_add_neg44âŸ©,
    âŸ¨_, sub_to_add_neg43âŸ©,
    âŸ¨_, sub_to_add_neg42âŸ©,
    âŸ¨_, sub_to_add_neg41âŸ©,
    âŸ¨_, sub_to_add_neg40âŸ©,
    âŸ¨_, sub_to_add_neg39âŸ©,
    âŸ¨_, sub_to_add_neg38âŸ©,
    âŸ¨_, sub_to_add_neg37âŸ©,
    âŸ¨_, sub_to_add_neg36âŸ©,
    âŸ¨_, sub_to_add_neg35âŸ©,
    âŸ¨_, sub_to_add_neg34âŸ©,
    âŸ¨_, sub_to_add_neg33âŸ©,
    âŸ¨_, sub_to_add_neg32âŸ©,
    âŸ¨_, sub_to_add_neg31âŸ©,
    âŸ¨_, sub_to_add_neg30âŸ©,
    âŸ¨_, sub_to_add_neg29âŸ©,
    âŸ¨_, sub_to_add_neg28âŸ©,
    âŸ¨_, sub_to_add_neg27âŸ©,
    âŸ¨_, sub_to_add_neg26âŸ©,
    âŸ¨_, sub_to_add_neg25âŸ©,
    âŸ¨_, sub_to_add_neg24âŸ©,
    âŸ¨_, sub_to_add_neg23âŸ©,
    âŸ¨_, sub_to_add_neg22âŸ©,
    âŸ¨_, sub_to_add_neg21âŸ©,
    âŸ¨_, sub_to_add_neg20âŸ©,
    âŸ¨_, sub_to_add_neg19âŸ©,
    âŸ¨_, sub_to_add_neg18âŸ©,
    âŸ¨_, sub_to_add_neg17âŸ©,
    âŸ¨_, sub_to_add_neg16âŸ©,
    âŸ¨_, sub_to_add_neg15âŸ©,
    âŸ¨_, sub_to_add_neg14âŸ©,
    âŸ¨_, sub_to_add_neg13âŸ©,
    âŸ¨_, sub_to_add_neg12âŸ©,
    âŸ¨_, sub_to_add_neg11âŸ©,
    âŸ¨_, sub_to_add_neg10âŸ©,
    âŸ¨_, sub_to_add_neg9âŸ©,
    âŸ¨_, sub_to_add_neg8âŸ©,
    âŸ¨_, sub_to_add_neg7âŸ©,
    âŸ¨_, sub_to_add_neg6âŸ©,
    âŸ¨_, sub_to_add_neg5âŸ©,
    âŸ¨_, sub_to_add_neg4âŸ©,
    âŸ¨_, sub_to_add_neg3âŸ©,
    âŸ¨_, sub_to_add_neg2âŸ©,
    âŸ¨_, sub_to_add_neg1âŸ©,
    âŸ¨_, sub_to_add_0âŸ©,
    âŸ¨_, sub_to_add_1âŸ©,
    âŸ¨_, sub_to_add_2âŸ©,
    âŸ¨_, sub_to_add_3âŸ©,
    âŸ¨_, sub_to_add_4âŸ©,
    âŸ¨_, sub_to_add_5âŸ©,
    âŸ¨_, sub_to_add_6âŸ©,
    âŸ¨_, sub_to_add_7âŸ©,
    âŸ¨_, sub_to_add_8âŸ©,
    âŸ¨_, sub_to_add_9âŸ©,
    âŸ¨_, sub_to_add_10âŸ©,
    âŸ¨_, sub_to_add_11âŸ©,
    âŸ¨_, sub_to_add_12âŸ©,
    âŸ¨_, sub_to_add_13âŸ©,
    âŸ¨_, sub_to_add_14âŸ©,
    âŸ¨_, sub_to_add_15âŸ©,
    âŸ¨_, sub_to_add_16âŸ©,
    âŸ¨_, sub_to_add_17âŸ©,
    âŸ¨_, sub_to_add_18âŸ©,
    âŸ¨_, sub_to_add_19âŸ©,
    âŸ¨_, sub_to_add_20âŸ©,
    âŸ¨_, sub_to_add_21âŸ©,
    âŸ¨_, sub_to_add_22âŸ©,
    âŸ¨_, sub_to_add_23âŸ©,
    âŸ¨_, sub_to_add_24âŸ©,
    âŸ¨_, sub_to_add_25âŸ©,
    âŸ¨_, sub_to_add_26âŸ©,
    âŸ¨_, sub_to_add_27âŸ©,
    âŸ¨_, sub_to_add_28âŸ©,
    âŸ¨_, sub_to_add_29âŸ©,
    âŸ¨_, sub_to_add_30âŸ©,
    âŸ¨_, sub_to_add_31âŸ©,
    âŸ¨_, sub_to_add_32âŸ©,
    âŸ¨_, sub_to_add_33âŸ©,
    âŸ¨_, sub_to_add_34âŸ©,
    âŸ¨_, sub_to_add_35âŸ©,
    âŸ¨_, sub_to_add_36âŸ©,
    âŸ¨_, sub_to_add_37âŸ©,
    âŸ¨_, sub_to_add_38âŸ©,
    âŸ¨_, sub_to_add_39âŸ©,
    âŸ¨_, sub_to_add_40âŸ©,
    âŸ¨_, sub_to_add_41âŸ©,
    âŸ¨_, sub_to_add_42âŸ©,
    âŸ¨_, sub_to_add_43âŸ©,
    âŸ¨_, sub_to_add_44âŸ©,
    âŸ¨_, sub_to_add_45âŸ©,
    âŸ¨_, sub_to_add_46âŸ©,
    âŸ¨_, sub_to_add_47âŸ©,
    âŸ¨_, sub_to_add_48âŸ©,
    âŸ¨_, sub_to_add_49âŸ©,
    âŸ¨_, sub_to_add_50âŸ©
  ]



/-! ### mul_to_shl -/

/--
Test the rewrite:
    (x * 2^n) â†’ (x << n)
-/

def mul_to_shl_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (0) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_4 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_8 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (3) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_16 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (16) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_32 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (32) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (5) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_64 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (64) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (6) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_128 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (128) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (7) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_256 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (256) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_512 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (512) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (9) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_1024 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1024) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (10) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_2048 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2048) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (11) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_4096 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4096) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (12) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_8192 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8192) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (13) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_16384 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (16384) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (14) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_32768 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (32768) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (15) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_65536 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (65536) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (16) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_131072 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (131072) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (17) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_262144 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (262144) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (18) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_524288 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (524288) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (19) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_1048576 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1048576) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (20) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_2097152 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2097152) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (21) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_4194304 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4194304) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (22) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_8388608 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8388608) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (23) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_16777216 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (16777216) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (24) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_33554432 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (33554432) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (25) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_67108864 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (67108864) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (26) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_134217728 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (134217728) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (27) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_268435456 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (268435456) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (28) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_536870912 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (536870912) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (29) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_1073741824 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1073741824) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (30) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_2147483648 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2147483648) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (31) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_4294967296 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4294967296) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (32) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_8589934592 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8589934592) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (33) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_17179869184 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (17179869184) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (34) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_34359738368 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (34359738368) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (35) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_68719476736 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (68719476736) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (36) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_137438953472 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (137438953472) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (37) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_274877906944 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (274877906944) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (38) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_549755813888 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (549755813888) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (39) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_1099511627776 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1099511627776) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (40) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_2199023255552 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2199023255552) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (41) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_4398046511104 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4398046511104) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (42) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_8796093022208 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8796093022208) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (43) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_17592186044416 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (17592186044416) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (44) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_35184372088832 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (35184372088832) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (45) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_70368744177664 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (70368744177664) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (46) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_140737488355328 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (140737488355328) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (47) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_281474976710656 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (281474976710656) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (48) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_562949953421312 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (562949953421312) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (49) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_1125899906842624 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1125899906842624) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (50) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_2251799813685248 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2251799813685248) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (51) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_4503599627370496 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4503599627370496) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (52) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_9007199254740992 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (9007199254740992) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (53) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_18014398509481984 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (18014398509481984) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (54) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_36028797018963968 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (36028797018963968) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (55) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_72057594037927936 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (72057594037927936) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (56) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_144115188075855872 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (144115188075855872) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (57) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_288230376151711744 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (288230376151711744) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (58) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_576460752303423488 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (576460752303423488) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (59) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_1152921504606846976 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1152921504606846976) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (60) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_2305843009213693952 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2305843009213693952) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (61) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_4611686018427387904 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4611686018427387904) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (62) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_9223372036854775808 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (9223372036854775808) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (63) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl : List (Î£ Î“, LLVMPeepholeRewriteRefine 64 Î“) :=
  [
    âŸ¨_, mul_to_shl_1âŸ©,
    âŸ¨_, mul_to_shl_2âŸ©,
    âŸ¨_, mul_to_shl_4âŸ©,
    âŸ¨_, mul_to_shl_8âŸ©,
    âŸ¨_, mul_to_shl_16âŸ©,
    âŸ¨_, mul_to_shl_32âŸ©,
    âŸ¨_, mul_to_shl_64âŸ©,
    âŸ¨_, mul_to_shl_128âŸ©,
    âŸ¨_, mul_to_shl_256âŸ©,
    âŸ¨_, mul_to_shl_512âŸ©,
    âŸ¨_, mul_to_shl_1024âŸ©,
    âŸ¨_, mul_to_shl_2048âŸ©,
    âŸ¨_, mul_to_shl_4096âŸ©,
    âŸ¨_, mul_to_shl_8192âŸ©,
    âŸ¨_, mul_to_shl_16384âŸ©,
    âŸ¨_, mul_to_shl_32768âŸ©,
    âŸ¨_, mul_to_shl_65536âŸ©,
    âŸ¨_, mul_to_shl_131072âŸ©,
    âŸ¨_, mul_to_shl_262144âŸ©,
    âŸ¨_, mul_to_shl_524288âŸ©,
    âŸ¨_, mul_to_shl_1048576âŸ©,
    âŸ¨_, mul_to_shl_2097152âŸ©,
    âŸ¨_, mul_to_shl_4194304âŸ©,
    âŸ¨_, mul_to_shl_8388608âŸ©,
    âŸ¨_, mul_to_shl_16777216âŸ©,
    âŸ¨_, mul_to_shl_33554432âŸ©,
    âŸ¨_, mul_to_shl_67108864âŸ©,
    âŸ¨_, mul_to_shl_134217728âŸ©,
    âŸ¨_, mul_to_shl_268435456âŸ©,
    âŸ¨_, mul_to_shl_536870912âŸ©,
    âŸ¨_, mul_to_shl_1073741824âŸ©,
    âŸ¨_, mul_to_shl_2147483648âŸ©,
    âŸ¨_, mul_to_shl_4294967296âŸ©,
    âŸ¨_, mul_to_shl_8589934592âŸ©,
    âŸ¨_, mul_to_shl_17179869184âŸ©,
    âŸ¨_, mul_to_shl_34359738368âŸ©,
    âŸ¨_, mul_to_shl_68719476736âŸ©,
    âŸ¨_, mul_to_shl_137438953472âŸ©,
    âŸ¨_, mul_to_shl_274877906944âŸ©,
    âŸ¨_, mul_to_shl_549755813888âŸ©,
    âŸ¨_, mul_to_shl_1099511627776âŸ©,
    âŸ¨_, mul_to_shl_2199023255552âŸ©,
    âŸ¨_, mul_to_shl_4398046511104âŸ©,
    âŸ¨_, mul_to_shl_8796093022208âŸ©,
    âŸ¨_, mul_to_shl_17592186044416âŸ©,
    âŸ¨_, mul_to_shl_35184372088832âŸ©,
    âŸ¨_, mul_to_shl_70368744177664âŸ©,
    âŸ¨_, mul_to_shl_140737488355328âŸ©,
    âŸ¨_, mul_to_shl_281474976710656âŸ©,
    âŸ¨_, mul_to_shl_562949953421312âŸ©,
    âŸ¨_, mul_to_shl_1125899906842624âŸ©,
    âŸ¨_, mul_to_shl_2251799813685248âŸ©,
    âŸ¨_, mul_to_shl_4503599627370496âŸ©,
    âŸ¨_, mul_to_shl_9007199254740992âŸ©,
    âŸ¨_, mul_to_shl_18014398509481984âŸ©,
    âŸ¨_, mul_to_shl_36028797018963968âŸ©,
    âŸ¨_, mul_to_shl_72057594037927936âŸ©,
    âŸ¨_, mul_to_shl_144115188075855872âŸ©,
    âŸ¨_, mul_to_shl_288230376151711744âŸ©,
    âŸ¨_, mul_to_shl_576460752303423488âŸ©,
    âŸ¨_, mul_to_shl_1152921504606846976âŸ©,
    âŸ¨_, mul_to_shl_2305843009213693952âŸ©,
    âŸ¨_, mul_to_shl_4611686018427387904âŸ©,
    âŸ¨_, mul_to_shl_9223372036854775808âŸ©
  ]



/-! ### urem_pow2_to_mask -/

/--
Test the rewrite:
    urem(x, 2^n) -> x & (2^n - 1)
-/

def urem_pow2_to_mask_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (0) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_4 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (3) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_8 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (7) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_16 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (16) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (15) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_32 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (32) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (31) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_64 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (64) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (63) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_128 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (128) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (127) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_256 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (256) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (255) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_512 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (512) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (511) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_1024 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1024) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1023) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_2048 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2048) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2047) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_4096 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4096) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4095) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_8192 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8192) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8191) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_16384 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (16384) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (16383) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_32768 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (32768) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (32767) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_65536 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (65536) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (65535) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_131072 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (131072) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (131071) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_262144 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (262144) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (262143) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_524288 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (524288) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (524287) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_1048576 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1048576) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1048575) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_2097152 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2097152) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2097151) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_4194304 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4194304) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4194303) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_8388608 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8388608) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8388607) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_16777216 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (16777216) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (16777215) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_33554432 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (33554432) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (33554431) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_67108864 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (67108864) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (67108863) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_134217728 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (134217728) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (134217727) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_268435456 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (268435456) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (268435455) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_536870912 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (536870912) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (536870911) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_1073741824 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1073741824) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1073741823) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_2147483648 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2147483648) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2147483647) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_4294967296 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4294967296) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4294967295) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_8589934592 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8589934592) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8589934591) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_17179869184 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (17179869184) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (17179869183) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_34359738368 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (34359738368) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (34359738367) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_68719476736 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (68719476736) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (68719476735) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_137438953472 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (137438953472) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (137438953471) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_274877906944 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (274877906944) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (274877906943) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_549755813888 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (549755813888) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (549755813887) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_1099511627776 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1099511627776) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1099511627775) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_2199023255552 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2199023255552) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2199023255551) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_4398046511104 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4398046511104) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4398046511103) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_8796093022208 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8796093022208) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8796093022207) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_17592186044416 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (17592186044416) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (17592186044415) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_35184372088832 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (35184372088832) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (35184372088831) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_70368744177664 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (70368744177664) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (70368744177663) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_140737488355328 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (140737488355328) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (140737488355327) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_281474976710656 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (281474976710656) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (281474976710655) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_562949953421312 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (562949953421312) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (562949953421311) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_1125899906842624 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1125899906842624) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1125899906842623) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_2251799813685248 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2251799813685248) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2251799813685247) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_4503599627370496 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4503599627370496) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4503599627370495) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_9007199254740992 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (9007199254740992) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (9007199254740991) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_18014398509481984 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (18014398509481984) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (18014398509481983) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_36028797018963968 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (36028797018963968) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (36028797018963967) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_72057594037927936 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (72057594037927936) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (72057594037927935) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_144115188075855872 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (144115188075855872) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (144115188075855871) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_288230376151711744 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (288230376151711744) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (288230376151711743) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_576460752303423488 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (576460752303423488) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (576460752303423487) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_1152921504606846976 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1152921504606846976) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1152921504606846975) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_2305843009213693952 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2305843009213693952) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2305843009213693951) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_4611686018427387904 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4611686018427387904) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4611686018427387903) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_9223372036854775808 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (9223372036854775808) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (9223372036854775807) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask : List (Î£ Î“, LLVMPeepholeRewriteRefine 64 Î“) :=
  [
    âŸ¨_, urem_pow2_to_mask_1âŸ©,
    âŸ¨_, urem_pow2_to_mask_2âŸ©,
    âŸ¨_, urem_pow2_to_mask_4âŸ©,
    âŸ¨_, urem_pow2_to_mask_8âŸ©,
    âŸ¨_, urem_pow2_to_mask_16âŸ©,
    âŸ¨_, urem_pow2_to_mask_32âŸ©,
    âŸ¨_, urem_pow2_to_mask_64âŸ©,
    âŸ¨_, urem_pow2_to_mask_128âŸ©,
    âŸ¨_, urem_pow2_to_mask_256âŸ©,
    âŸ¨_, urem_pow2_to_mask_512âŸ©,
    âŸ¨_, urem_pow2_to_mask_1024âŸ©,
    âŸ¨_, urem_pow2_to_mask_2048âŸ©,
    âŸ¨_, urem_pow2_to_mask_4096âŸ©,
    âŸ¨_, urem_pow2_to_mask_8192âŸ©,
    âŸ¨_, urem_pow2_to_mask_16384âŸ©,
    âŸ¨_, urem_pow2_to_mask_32768âŸ©,
    âŸ¨_, urem_pow2_to_mask_65536âŸ©,
    âŸ¨_, urem_pow2_to_mask_131072âŸ©,
    âŸ¨_, urem_pow2_to_mask_262144âŸ©,
    âŸ¨_, urem_pow2_to_mask_524288âŸ©,
    âŸ¨_, urem_pow2_to_mask_1048576âŸ©,
    âŸ¨_, urem_pow2_to_mask_2097152âŸ©,
    âŸ¨_, urem_pow2_to_mask_4194304âŸ©,
    âŸ¨_, urem_pow2_to_mask_8388608âŸ©,
    âŸ¨_, urem_pow2_to_mask_16777216âŸ©,
    âŸ¨_, urem_pow2_to_mask_33554432âŸ©,
    âŸ¨_, urem_pow2_to_mask_67108864âŸ©,
    âŸ¨_, urem_pow2_to_mask_134217728âŸ©,
    âŸ¨_, urem_pow2_to_mask_268435456âŸ©,
    âŸ¨_, urem_pow2_to_mask_536870912âŸ©,
    âŸ¨_, urem_pow2_to_mask_1073741824âŸ©,
    âŸ¨_, urem_pow2_to_mask_2147483648âŸ©,
    âŸ¨_, urem_pow2_to_mask_4294967296âŸ©,
    âŸ¨_, urem_pow2_to_mask_8589934592âŸ©,
    âŸ¨_, urem_pow2_to_mask_17179869184âŸ©,
    âŸ¨_, urem_pow2_to_mask_34359738368âŸ©,
    âŸ¨_, urem_pow2_to_mask_68719476736âŸ©,
    âŸ¨_, urem_pow2_to_mask_137438953472âŸ©,
    âŸ¨_, urem_pow2_to_mask_274877906944âŸ©,
    âŸ¨_, urem_pow2_to_mask_549755813888âŸ©,
    âŸ¨_, urem_pow2_to_mask_1099511627776âŸ©,
    âŸ¨_, urem_pow2_to_mask_2199023255552âŸ©,
    âŸ¨_, urem_pow2_to_mask_4398046511104âŸ©,
    âŸ¨_, urem_pow2_to_mask_8796093022208âŸ©,
    âŸ¨_, urem_pow2_to_mask_17592186044416âŸ©,
    âŸ¨_, urem_pow2_to_mask_35184372088832âŸ©,
    âŸ¨_, urem_pow2_to_mask_70368744177664âŸ©,
    âŸ¨_, urem_pow2_to_mask_140737488355328âŸ©,
    âŸ¨_, urem_pow2_to_mask_281474976710656âŸ©,
    âŸ¨_, urem_pow2_to_mask_562949953421312âŸ©,
    âŸ¨_, urem_pow2_to_mask_1125899906842624âŸ©,
    âŸ¨_, urem_pow2_to_mask_2251799813685248âŸ©,
    âŸ¨_, urem_pow2_to_mask_4503599627370496âŸ©,
    âŸ¨_, urem_pow2_to_mask_9007199254740992âŸ©,
    âŸ¨_, urem_pow2_to_mask_18014398509481984âŸ©,
    âŸ¨_, urem_pow2_to_mask_36028797018963968âŸ©,
    âŸ¨_, urem_pow2_to_mask_72057594037927936âŸ©,
    âŸ¨_, urem_pow2_to_mask_144115188075855872âŸ©,
    âŸ¨_, urem_pow2_to_mask_288230376151711744âŸ©,
    âŸ¨_, urem_pow2_to_mask_576460752303423488âŸ©,
    âŸ¨_, urem_pow2_to_mask_1152921504606846976âŸ©,
    âŸ¨_, urem_pow2_to_mask_2305843009213693952âŸ©,
    âŸ¨_, urem_pow2_to_mask_4611686018427387904âŸ©,
    âŸ¨_, urem_pow2_to_mask_9223372036854775808âŸ©
  ]



/-! ### cast_combines_narrow_binops -/

/--
Test the rewrite:
    trunc (binop X, C) --> binop (trunc X, trunc C)
-/
def narrow_binop_add_neg50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_0 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_0 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_0 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_0 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_0 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_0 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def cast_combines_narrow_binops : List (Î£ Î“, LLVMPeepholeRewriteRefine 64 Î“) :=
  [
    âŸ¨_, narrow_binop_add_neg50âŸ©,
    âŸ¨_, narrow_binop_sub_neg50âŸ©,
    âŸ¨_, narrow_binop_mul_neg50âŸ©,
    âŸ¨_, narrow_binop_and_neg50âŸ©,
    âŸ¨_, narrow_binop_or_neg50âŸ©,
    âŸ¨_, narrow_binop_xor_neg50âŸ©,
    âŸ¨_, narrow_binop_add_neg49âŸ©,
    âŸ¨_, narrow_binop_sub_neg49âŸ©,
    âŸ¨_, narrow_binop_mul_neg49âŸ©,
    âŸ¨_, narrow_binop_and_neg49âŸ©,
    âŸ¨_, narrow_binop_or_neg49âŸ©,
    âŸ¨_, narrow_binop_xor_neg49âŸ©,
    âŸ¨_, narrow_binop_add_neg48âŸ©,
    âŸ¨_, narrow_binop_sub_neg48âŸ©,
    âŸ¨_, narrow_binop_mul_neg48âŸ©,
    âŸ¨_, narrow_binop_and_neg48âŸ©,
    âŸ¨_, narrow_binop_or_neg48âŸ©,
    âŸ¨_, narrow_binop_xor_neg48âŸ©,
    âŸ¨_, narrow_binop_add_neg47âŸ©,
    âŸ¨_, narrow_binop_sub_neg47âŸ©,
    âŸ¨_, narrow_binop_mul_neg47âŸ©,
    âŸ¨_, narrow_binop_and_neg47âŸ©,
    âŸ¨_, narrow_binop_or_neg47âŸ©,
    âŸ¨_, narrow_binop_xor_neg47âŸ©,
    âŸ¨_, narrow_binop_add_neg46âŸ©,
    âŸ¨_, narrow_binop_sub_neg46âŸ©,
    âŸ¨_, narrow_binop_mul_neg46âŸ©,
    âŸ¨_, narrow_binop_and_neg46âŸ©,
    âŸ¨_, narrow_binop_or_neg46âŸ©,
    âŸ¨_, narrow_binop_xor_neg46âŸ©,
    âŸ¨_, narrow_binop_add_neg45âŸ©,
    âŸ¨_, narrow_binop_sub_neg45âŸ©,
    âŸ¨_, narrow_binop_mul_neg45âŸ©,
    âŸ¨_, narrow_binop_and_neg45âŸ©,
    âŸ¨_, narrow_binop_or_neg45âŸ©,
    âŸ¨_, narrow_binop_xor_neg45âŸ©,
    âŸ¨_, narrow_binop_add_neg44âŸ©,
    âŸ¨_, narrow_binop_sub_neg44âŸ©,
    âŸ¨_, narrow_binop_mul_neg44âŸ©,
    âŸ¨_, narrow_binop_and_neg44âŸ©,
    âŸ¨_, narrow_binop_or_neg44âŸ©,
    âŸ¨_, narrow_binop_xor_neg44âŸ©,
    âŸ¨_, narrow_binop_add_neg43âŸ©,
    âŸ¨_, narrow_binop_sub_neg43âŸ©,
    âŸ¨_, narrow_binop_mul_neg43âŸ©,
    âŸ¨_, narrow_binop_and_neg43âŸ©,
    âŸ¨_, narrow_binop_or_neg43âŸ©,
    âŸ¨_, narrow_binop_xor_neg43âŸ©,
    âŸ¨_, narrow_binop_add_neg42âŸ©,
    âŸ¨_, narrow_binop_sub_neg42âŸ©,
    âŸ¨_, narrow_binop_mul_neg42âŸ©,
    âŸ¨_, narrow_binop_and_neg42âŸ©,
    âŸ¨_, narrow_binop_or_neg42âŸ©,
    âŸ¨_, narrow_binop_xor_neg42âŸ©,
    âŸ¨_, narrow_binop_add_neg41âŸ©,
    âŸ¨_, narrow_binop_sub_neg41âŸ©,
    âŸ¨_, narrow_binop_mul_neg41âŸ©,
    âŸ¨_, narrow_binop_and_neg41âŸ©,
    âŸ¨_, narrow_binop_or_neg41âŸ©,
    âŸ¨_, narrow_binop_xor_neg41âŸ©,
    âŸ¨_, narrow_binop_add_neg40âŸ©,
    âŸ¨_, narrow_binop_sub_neg40âŸ©,
    âŸ¨_, narrow_binop_mul_neg40âŸ©,
    âŸ¨_, narrow_binop_and_neg40âŸ©,
    âŸ¨_, narrow_binop_or_neg40âŸ©,
    âŸ¨_, narrow_binop_xor_neg40âŸ©,
    âŸ¨_, narrow_binop_add_neg39âŸ©,
    âŸ¨_, narrow_binop_sub_neg39âŸ©,
    âŸ¨_, narrow_binop_mul_neg39âŸ©,
    âŸ¨_, narrow_binop_and_neg39âŸ©,
    âŸ¨_, narrow_binop_or_neg39âŸ©,
    âŸ¨_, narrow_binop_xor_neg39âŸ©,
    âŸ¨_, narrow_binop_add_neg38âŸ©,
    âŸ¨_, narrow_binop_sub_neg38âŸ©,
    âŸ¨_, narrow_binop_mul_neg38âŸ©,
    âŸ¨_, narrow_binop_and_neg38âŸ©,
    âŸ¨_, narrow_binop_or_neg38âŸ©,
    âŸ¨_, narrow_binop_xor_neg38âŸ©,
    âŸ¨_, narrow_binop_add_neg37âŸ©,
    âŸ¨_, narrow_binop_sub_neg37âŸ©,
    âŸ¨_, narrow_binop_mul_neg37âŸ©,
    âŸ¨_, narrow_binop_and_neg37âŸ©,
    âŸ¨_, narrow_binop_or_neg37âŸ©,
    âŸ¨_, narrow_binop_xor_neg37âŸ©,
    âŸ¨_, narrow_binop_add_neg36âŸ©,
    âŸ¨_, narrow_binop_sub_neg36âŸ©,
    âŸ¨_, narrow_binop_mul_neg36âŸ©,
    âŸ¨_, narrow_binop_and_neg36âŸ©,
    âŸ¨_, narrow_binop_or_neg36âŸ©,
    âŸ¨_, narrow_binop_xor_neg36âŸ©,
    âŸ¨_, narrow_binop_add_neg35âŸ©,
    âŸ¨_, narrow_binop_sub_neg35âŸ©,
    âŸ¨_, narrow_binop_mul_neg35âŸ©,
    âŸ¨_, narrow_binop_and_neg35âŸ©,
    âŸ¨_, narrow_binop_or_neg35âŸ©,
    âŸ¨_, narrow_binop_xor_neg35âŸ©,
    âŸ¨_, narrow_binop_add_neg34âŸ©,
    âŸ¨_, narrow_binop_sub_neg34âŸ©,
    âŸ¨_, narrow_binop_mul_neg34âŸ©,
    âŸ¨_, narrow_binop_and_neg34âŸ©,
    âŸ¨_, narrow_binop_or_neg34âŸ©,
    âŸ¨_, narrow_binop_xor_neg34âŸ©,
    âŸ¨_, narrow_binop_add_neg33âŸ©,
    âŸ¨_, narrow_binop_sub_neg33âŸ©,
    âŸ¨_, narrow_binop_mul_neg33âŸ©,
    âŸ¨_, narrow_binop_and_neg33âŸ©,
    âŸ¨_, narrow_binop_or_neg33âŸ©,
    âŸ¨_, narrow_binop_xor_neg33âŸ©,
    âŸ¨_, narrow_binop_add_neg32âŸ©,
    âŸ¨_, narrow_binop_sub_neg32âŸ©,
    âŸ¨_, narrow_binop_mul_neg32âŸ©,
    âŸ¨_, narrow_binop_and_neg32âŸ©,
    âŸ¨_, narrow_binop_or_neg32âŸ©,
    âŸ¨_, narrow_binop_xor_neg32âŸ©,
    âŸ¨_, narrow_binop_add_neg31âŸ©,
    âŸ¨_, narrow_binop_sub_neg31âŸ©,
    âŸ¨_, narrow_binop_mul_neg31âŸ©,
    âŸ¨_, narrow_binop_and_neg31âŸ©,
    âŸ¨_, narrow_binop_or_neg31âŸ©,
    âŸ¨_, narrow_binop_xor_neg31âŸ©,
    âŸ¨_, narrow_binop_add_neg30âŸ©,
    âŸ¨_, narrow_binop_sub_neg30âŸ©,
    âŸ¨_, narrow_binop_mul_neg30âŸ©,
    âŸ¨_, narrow_binop_and_neg30âŸ©,
    âŸ¨_, narrow_binop_or_neg30âŸ©,
    âŸ¨_, narrow_binop_xor_neg30âŸ©,
    âŸ¨_, narrow_binop_add_neg29âŸ©,
    âŸ¨_, narrow_binop_sub_neg29âŸ©,
    âŸ¨_, narrow_binop_mul_neg29âŸ©,
    âŸ¨_, narrow_binop_and_neg29âŸ©,
    âŸ¨_, narrow_binop_or_neg29âŸ©,
    âŸ¨_, narrow_binop_xor_neg29âŸ©,
    âŸ¨_, narrow_binop_add_neg28âŸ©,
    âŸ¨_, narrow_binop_sub_neg28âŸ©,
    âŸ¨_, narrow_binop_mul_neg28âŸ©,
    âŸ¨_, narrow_binop_and_neg28âŸ©,
    âŸ¨_, narrow_binop_or_neg28âŸ©,
    âŸ¨_, narrow_binop_xor_neg28âŸ©,
    âŸ¨_, narrow_binop_add_neg27âŸ©,
    âŸ¨_, narrow_binop_sub_neg27âŸ©,
    âŸ¨_, narrow_binop_mul_neg27âŸ©,
    âŸ¨_, narrow_binop_and_neg27âŸ©,
    âŸ¨_, narrow_binop_or_neg27âŸ©,
    âŸ¨_, narrow_binop_xor_neg27âŸ©,
    âŸ¨_, narrow_binop_add_neg26âŸ©,
    âŸ¨_, narrow_binop_sub_neg26âŸ©,
    âŸ¨_, narrow_binop_mul_neg26âŸ©,
    âŸ¨_, narrow_binop_and_neg26âŸ©,
    âŸ¨_, narrow_binop_or_neg26âŸ©,
    âŸ¨_, narrow_binop_xor_neg26âŸ©,
    âŸ¨_, narrow_binop_add_neg25âŸ©,
    âŸ¨_, narrow_binop_sub_neg25âŸ©,
    âŸ¨_, narrow_binop_mul_neg25âŸ©,
    âŸ¨_, narrow_binop_and_neg25âŸ©,
    âŸ¨_, narrow_binop_or_neg25âŸ©,
    âŸ¨_, narrow_binop_xor_neg25âŸ©,
    âŸ¨_, narrow_binop_add_neg24âŸ©,
    âŸ¨_, narrow_binop_sub_neg24âŸ©,
    âŸ¨_, narrow_binop_mul_neg24âŸ©,
    âŸ¨_, narrow_binop_and_neg24âŸ©,
    âŸ¨_, narrow_binop_or_neg24âŸ©,
    âŸ¨_, narrow_binop_xor_neg24âŸ©,
    âŸ¨_, narrow_binop_add_neg23âŸ©,
    âŸ¨_, narrow_binop_sub_neg23âŸ©,
    âŸ¨_, narrow_binop_mul_neg23âŸ©,
    âŸ¨_, narrow_binop_and_neg23âŸ©,
    âŸ¨_, narrow_binop_or_neg23âŸ©,
    âŸ¨_, narrow_binop_xor_neg23âŸ©,
    âŸ¨_, narrow_binop_add_neg22âŸ©,
    âŸ¨_, narrow_binop_sub_neg22âŸ©,
    âŸ¨_, narrow_binop_mul_neg22âŸ©,
    âŸ¨_, narrow_binop_and_neg22âŸ©,
    âŸ¨_, narrow_binop_or_neg22âŸ©,
    âŸ¨_, narrow_binop_xor_neg22âŸ©,
    âŸ¨_, narrow_binop_add_neg21âŸ©,
    âŸ¨_, narrow_binop_sub_neg21âŸ©,
    âŸ¨_, narrow_binop_mul_neg21âŸ©,
    âŸ¨_, narrow_binop_and_neg21âŸ©,
    âŸ¨_, narrow_binop_or_neg21âŸ©,
    âŸ¨_, narrow_binop_xor_neg21âŸ©,
    âŸ¨_, narrow_binop_add_neg20âŸ©,
    âŸ¨_, narrow_binop_sub_neg20âŸ©,
    âŸ¨_, narrow_binop_mul_neg20âŸ©,
    âŸ¨_, narrow_binop_and_neg20âŸ©,
    âŸ¨_, narrow_binop_or_neg20âŸ©,
    âŸ¨_, narrow_binop_xor_neg20âŸ©,
    âŸ¨_, narrow_binop_add_neg19âŸ©,
    âŸ¨_, narrow_binop_sub_neg19âŸ©,
    âŸ¨_, narrow_binop_mul_neg19âŸ©,
    âŸ¨_, narrow_binop_and_neg19âŸ©,
    âŸ¨_, narrow_binop_or_neg19âŸ©,
    âŸ¨_, narrow_binop_xor_neg19âŸ©,
    âŸ¨_, narrow_binop_add_neg18âŸ©,
    âŸ¨_, narrow_binop_sub_neg18âŸ©,
    âŸ¨_, narrow_binop_mul_neg18âŸ©,
    âŸ¨_, narrow_binop_and_neg18âŸ©,
    âŸ¨_, narrow_binop_or_neg18âŸ©,
    âŸ¨_, narrow_binop_xor_neg18âŸ©,
    âŸ¨_, narrow_binop_add_neg17âŸ©,
    âŸ¨_, narrow_binop_sub_neg17âŸ©,
    âŸ¨_, narrow_binop_mul_neg17âŸ©,
    âŸ¨_, narrow_binop_and_neg17âŸ©,
    âŸ¨_, narrow_binop_or_neg17âŸ©,
    âŸ¨_, narrow_binop_xor_neg17âŸ©,
    âŸ¨_, narrow_binop_add_neg16âŸ©,
    âŸ¨_, narrow_binop_sub_neg16âŸ©,
    âŸ¨_, narrow_binop_mul_neg16âŸ©,
    âŸ¨_, narrow_binop_and_neg16âŸ©,
    âŸ¨_, narrow_binop_or_neg16âŸ©,
    âŸ¨_, narrow_binop_xor_neg16âŸ©,
    âŸ¨_, narrow_binop_add_neg15âŸ©,
    âŸ¨_, narrow_binop_sub_neg15âŸ©,
    âŸ¨_, narrow_binop_mul_neg15âŸ©,
    âŸ¨_, narrow_binop_and_neg15âŸ©,
    âŸ¨_, narrow_binop_or_neg15âŸ©,
    âŸ¨_, narrow_binop_xor_neg15âŸ©,
    âŸ¨_, narrow_binop_add_neg14âŸ©,
    âŸ¨_, narrow_binop_sub_neg14âŸ©,
    âŸ¨_, narrow_binop_mul_neg14âŸ©,
    âŸ¨_, narrow_binop_and_neg14âŸ©,
    âŸ¨_, narrow_binop_or_neg14âŸ©,
    âŸ¨_, narrow_binop_xor_neg14âŸ©,
    âŸ¨_, narrow_binop_add_neg13âŸ©,
    âŸ¨_, narrow_binop_sub_neg13âŸ©,
    âŸ¨_, narrow_binop_mul_neg13âŸ©,
    âŸ¨_, narrow_binop_and_neg13âŸ©,
    âŸ¨_, narrow_binop_or_neg13âŸ©,
    âŸ¨_, narrow_binop_xor_neg13âŸ©,
    âŸ¨_, narrow_binop_add_neg12âŸ©,
    âŸ¨_, narrow_binop_sub_neg12âŸ©,
    âŸ¨_, narrow_binop_mul_neg12âŸ©,
    âŸ¨_, narrow_binop_and_neg12âŸ©,
    âŸ¨_, narrow_binop_or_neg12âŸ©,
    âŸ¨_, narrow_binop_xor_neg12âŸ©,
    âŸ¨_, narrow_binop_add_neg11âŸ©,
    âŸ¨_, narrow_binop_sub_neg11âŸ©,
    âŸ¨_, narrow_binop_mul_neg11âŸ©,
    âŸ¨_, narrow_binop_and_neg11âŸ©,
    âŸ¨_, narrow_binop_or_neg11âŸ©,
    âŸ¨_, narrow_binop_xor_neg11âŸ©,
    âŸ¨_, narrow_binop_add_neg10âŸ©,
    âŸ¨_, narrow_binop_sub_neg10âŸ©,
    âŸ¨_, narrow_binop_mul_neg10âŸ©,
    âŸ¨_, narrow_binop_and_neg10âŸ©,
    âŸ¨_, narrow_binop_or_neg10âŸ©,
    âŸ¨_, narrow_binop_xor_neg10âŸ©,
    âŸ¨_, narrow_binop_add_neg9âŸ©,
    âŸ¨_, narrow_binop_sub_neg9âŸ©,
    âŸ¨_, narrow_binop_mul_neg9âŸ©,
    âŸ¨_, narrow_binop_and_neg9âŸ©,
    âŸ¨_, narrow_binop_or_neg9âŸ©,
    âŸ¨_, narrow_binop_xor_neg9âŸ©,
    âŸ¨_, narrow_binop_add_neg8âŸ©,
    âŸ¨_, narrow_binop_sub_neg8âŸ©,
    âŸ¨_, narrow_binop_mul_neg8âŸ©,
    âŸ¨_, narrow_binop_and_neg8âŸ©,
    âŸ¨_, narrow_binop_or_neg8âŸ©,
    âŸ¨_, narrow_binop_xor_neg8âŸ©,
    âŸ¨_, narrow_binop_add_neg7âŸ©,
    âŸ¨_, narrow_binop_sub_neg7âŸ©,
    âŸ¨_, narrow_binop_mul_neg7âŸ©,
    âŸ¨_, narrow_binop_and_neg7âŸ©,
    âŸ¨_, narrow_binop_or_neg7âŸ©,
    âŸ¨_, narrow_binop_xor_neg7âŸ©,
    âŸ¨_, narrow_binop_add_neg6âŸ©,
    âŸ¨_, narrow_binop_sub_neg6âŸ©,
    âŸ¨_, narrow_binop_mul_neg6âŸ©,
    âŸ¨_, narrow_binop_and_neg6âŸ©,
    âŸ¨_, narrow_binop_or_neg6âŸ©,
    âŸ¨_, narrow_binop_xor_neg6âŸ©,
    âŸ¨_, narrow_binop_add_neg5âŸ©,
    âŸ¨_, narrow_binop_sub_neg5âŸ©,
    âŸ¨_, narrow_binop_mul_neg5âŸ©,
    âŸ¨_, narrow_binop_and_neg5âŸ©,
    âŸ¨_, narrow_binop_or_neg5âŸ©,
    âŸ¨_, narrow_binop_xor_neg5âŸ©,
    âŸ¨_, narrow_binop_add_neg4âŸ©,
    âŸ¨_, narrow_binop_sub_neg4âŸ©,
    âŸ¨_, narrow_binop_mul_neg4âŸ©,
    âŸ¨_, narrow_binop_and_neg4âŸ©,
    âŸ¨_, narrow_binop_or_neg4âŸ©,
    âŸ¨_, narrow_binop_xor_neg4âŸ©,
    âŸ¨_, narrow_binop_add_neg3âŸ©,
    âŸ¨_, narrow_binop_sub_neg3âŸ©,
    âŸ¨_, narrow_binop_mul_neg3âŸ©,
    âŸ¨_, narrow_binop_and_neg3âŸ©,
    âŸ¨_, narrow_binop_or_neg3âŸ©,
    âŸ¨_, narrow_binop_xor_neg3âŸ©,
    âŸ¨_, narrow_binop_add_neg2âŸ©,
    âŸ¨_, narrow_binop_sub_neg2âŸ©,
    âŸ¨_, narrow_binop_mul_neg2âŸ©,
    âŸ¨_, narrow_binop_and_neg2âŸ©,
    âŸ¨_, narrow_binop_or_neg2âŸ©,
    âŸ¨_, narrow_binop_xor_neg2âŸ©,
    âŸ¨_, narrow_binop_add_neg1âŸ©,
    âŸ¨_, narrow_binop_sub_neg1âŸ©,
    âŸ¨_, narrow_binop_mul_neg1âŸ©,
    âŸ¨_, narrow_binop_and_neg1âŸ©,
    âŸ¨_, narrow_binop_or_neg1âŸ©,
    âŸ¨_, narrow_binop_xor_neg1âŸ©,
    âŸ¨_, narrow_binop_add_0âŸ©,
    âŸ¨_, narrow_binop_sub_0âŸ©,
    âŸ¨_, narrow_binop_mul_0âŸ©,
    âŸ¨_, narrow_binop_and_0âŸ©,
    âŸ¨_, narrow_binop_or_0âŸ©,
    âŸ¨_, narrow_binop_xor_0âŸ©,
    âŸ¨_, narrow_binop_add_1âŸ©,
    âŸ¨_, narrow_binop_sub_1âŸ©,
    âŸ¨_, narrow_binop_mul_1âŸ©,
    âŸ¨_, narrow_binop_and_1âŸ©,
    âŸ¨_, narrow_binop_or_1âŸ©,
    âŸ¨_, narrow_binop_xor_1âŸ©,
    âŸ¨_, narrow_binop_add_2âŸ©,
    âŸ¨_, narrow_binop_sub_2âŸ©,
    âŸ¨_, narrow_binop_mul_2âŸ©,
    âŸ¨_, narrow_binop_and_2âŸ©,
    âŸ¨_, narrow_binop_or_2âŸ©,
    âŸ¨_, narrow_binop_xor_2âŸ©,
    âŸ¨_, narrow_binop_add_3âŸ©,
    âŸ¨_, narrow_binop_sub_3âŸ©,
    âŸ¨_, narrow_binop_mul_3âŸ©,
    âŸ¨_, narrow_binop_and_3âŸ©,
    âŸ¨_, narrow_binop_or_3âŸ©,
    âŸ¨_, narrow_binop_xor_3âŸ©,
    âŸ¨_, narrow_binop_add_4âŸ©,
    âŸ¨_, narrow_binop_sub_4âŸ©,
    âŸ¨_, narrow_binop_mul_4âŸ©,
    âŸ¨_, narrow_binop_and_4âŸ©,
    âŸ¨_, narrow_binop_or_4âŸ©,
    âŸ¨_, narrow_binop_xor_4âŸ©,
    âŸ¨_, narrow_binop_add_5âŸ©,
    âŸ¨_, narrow_binop_sub_5âŸ©,
    âŸ¨_, narrow_binop_mul_5âŸ©,
    âŸ¨_, narrow_binop_and_5âŸ©,
    âŸ¨_, narrow_binop_or_5âŸ©,
    âŸ¨_, narrow_binop_xor_5âŸ©,
    âŸ¨_, narrow_binop_add_6âŸ©,
    âŸ¨_, narrow_binop_sub_6âŸ©,
    âŸ¨_, narrow_binop_mul_6âŸ©,
    âŸ¨_, narrow_binop_and_6âŸ©,
    âŸ¨_, narrow_binop_or_6âŸ©,
    âŸ¨_, narrow_binop_xor_6âŸ©,
    âŸ¨_, narrow_binop_add_7âŸ©,
    âŸ¨_, narrow_binop_sub_7âŸ©,
    âŸ¨_, narrow_binop_mul_7âŸ©,
    âŸ¨_, narrow_binop_and_7âŸ©,
    âŸ¨_, narrow_binop_or_7âŸ©,
    âŸ¨_, narrow_binop_xor_7âŸ©,
    âŸ¨_, narrow_binop_add_8âŸ©,
    âŸ¨_, narrow_binop_sub_8âŸ©,
    âŸ¨_, narrow_binop_mul_8âŸ©,
    âŸ¨_, narrow_binop_and_8âŸ©,
    âŸ¨_, narrow_binop_or_8âŸ©,
    âŸ¨_, narrow_binop_xor_8âŸ©,
    âŸ¨_, narrow_binop_add_9âŸ©,
    âŸ¨_, narrow_binop_sub_9âŸ©,
    âŸ¨_, narrow_binop_mul_9âŸ©,
    âŸ¨_, narrow_binop_and_9âŸ©,
    âŸ¨_, narrow_binop_or_9âŸ©,
    âŸ¨_, narrow_binop_xor_9âŸ©,
    âŸ¨_, narrow_binop_add_10âŸ©,
    âŸ¨_, narrow_binop_sub_10âŸ©,
    âŸ¨_, narrow_binop_mul_10âŸ©,
    âŸ¨_, narrow_binop_and_10âŸ©,
    âŸ¨_, narrow_binop_or_10âŸ©,
    âŸ¨_, narrow_binop_xor_10âŸ©,
    âŸ¨_, narrow_binop_add_11âŸ©,
    âŸ¨_, narrow_binop_sub_11âŸ©,
    âŸ¨_, narrow_binop_mul_11âŸ©,
    âŸ¨_, narrow_binop_and_11âŸ©,
    âŸ¨_, narrow_binop_or_11âŸ©,
    âŸ¨_, narrow_binop_xor_11âŸ©,
    âŸ¨_, narrow_binop_add_12âŸ©,
    âŸ¨_, narrow_binop_sub_12âŸ©,
    âŸ¨_, narrow_binop_mul_12âŸ©,
    âŸ¨_, narrow_binop_and_12âŸ©,
    âŸ¨_, narrow_binop_or_12âŸ©,
    âŸ¨_, narrow_binop_xor_12âŸ©,
    âŸ¨_, narrow_binop_add_13âŸ©,
    âŸ¨_, narrow_binop_sub_13âŸ©,
    âŸ¨_, narrow_binop_mul_13âŸ©,
    âŸ¨_, narrow_binop_and_13âŸ©,
    âŸ¨_, narrow_binop_or_13âŸ©,
    âŸ¨_, narrow_binop_xor_13âŸ©,
    âŸ¨_, narrow_binop_add_14âŸ©,
    âŸ¨_, narrow_binop_sub_14âŸ©,
    âŸ¨_, narrow_binop_mul_14âŸ©,
    âŸ¨_, narrow_binop_and_14âŸ©,
    âŸ¨_, narrow_binop_or_14âŸ©,
    âŸ¨_, narrow_binop_xor_14âŸ©,
    âŸ¨_, narrow_binop_add_15âŸ©,
    âŸ¨_, narrow_binop_sub_15âŸ©,
    âŸ¨_, narrow_binop_mul_15âŸ©,
    âŸ¨_, narrow_binop_and_15âŸ©,
    âŸ¨_, narrow_binop_or_15âŸ©,
    âŸ¨_, narrow_binop_xor_15âŸ©,
    âŸ¨_, narrow_binop_add_16âŸ©,
    âŸ¨_, narrow_binop_sub_16âŸ©,
    âŸ¨_, narrow_binop_mul_16âŸ©,
    âŸ¨_, narrow_binop_and_16âŸ©,
    âŸ¨_, narrow_binop_or_16âŸ©,
    âŸ¨_, narrow_binop_xor_16âŸ©,
    âŸ¨_, narrow_binop_add_17âŸ©,
    âŸ¨_, narrow_binop_sub_17âŸ©,
    âŸ¨_, narrow_binop_mul_17âŸ©,
    âŸ¨_, narrow_binop_and_17âŸ©,
    âŸ¨_, narrow_binop_or_17âŸ©,
    âŸ¨_, narrow_binop_xor_17âŸ©,
    âŸ¨_, narrow_binop_add_18âŸ©,
    âŸ¨_, narrow_binop_sub_18âŸ©,
    âŸ¨_, narrow_binop_mul_18âŸ©,
    âŸ¨_, narrow_binop_and_18âŸ©,
    âŸ¨_, narrow_binop_or_18âŸ©,
    âŸ¨_, narrow_binop_xor_18âŸ©,
    âŸ¨_, narrow_binop_add_19âŸ©,
    âŸ¨_, narrow_binop_sub_19âŸ©,
    âŸ¨_, narrow_binop_mul_19âŸ©,
    âŸ¨_, narrow_binop_and_19âŸ©,
    âŸ¨_, narrow_binop_or_19âŸ©,
    âŸ¨_, narrow_binop_xor_19âŸ©,
    âŸ¨_, narrow_binop_add_20âŸ©,
    âŸ¨_, narrow_binop_sub_20âŸ©,
    âŸ¨_, narrow_binop_mul_20âŸ©,
    âŸ¨_, narrow_binop_and_20âŸ©,
    âŸ¨_, narrow_binop_or_20âŸ©,
    âŸ¨_, narrow_binop_xor_20âŸ©,
    âŸ¨_, narrow_binop_add_21âŸ©,
    âŸ¨_, narrow_binop_sub_21âŸ©,
    âŸ¨_, narrow_binop_mul_21âŸ©,
    âŸ¨_, narrow_binop_and_21âŸ©,
    âŸ¨_, narrow_binop_or_21âŸ©,
    âŸ¨_, narrow_binop_xor_21âŸ©,
    âŸ¨_, narrow_binop_add_22âŸ©,
    âŸ¨_, narrow_binop_sub_22âŸ©,
    âŸ¨_, narrow_binop_mul_22âŸ©,
    âŸ¨_, narrow_binop_and_22âŸ©,
    âŸ¨_, narrow_binop_or_22âŸ©,
    âŸ¨_, narrow_binop_xor_22âŸ©,
    âŸ¨_, narrow_binop_add_23âŸ©,
    âŸ¨_, narrow_binop_sub_23âŸ©,
    âŸ¨_, narrow_binop_mul_23âŸ©,
    âŸ¨_, narrow_binop_and_23âŸ©,
    âŸ¨_, narrow_binop_or_23âŸ©,
    âŸ¨_, narrow_binop_xor_23âŸ©,
    âŸ¨_, narrow_binop_add_24âŸ©,
    âŸ¨_, narrow_binop_sub_24âŸ©,
    âŸ¨_, narrow_binop_mul_24âŸ©,
    âŸ¨_, narrow_binop_and_24âŸ©,
    âŸ¨_, narrow_binop_or_24âŸ©,
    âŸ¨_, narrow_binop_xor_24âŸ©,
    âŸ¨_, narrow_binop_add_25âŸ©,
    âŸ¨_, narrow_binop_sub_25âŸ©,
    âŸ¨_, narrow_binop_mul_25âŸ©,
    âŸ¨_, narrow_binop_and_25âŸ©,
    âŸ¨_, narrow_binop_or_25âŸ©,
    âŸ¨_, narrow_binop_xor_25âŸ©,
    âŸ¨_, narrow_binop_add_26âŸ©,
    âŸ¨_, narrow_binop_sub_26âŸ©,
    âŸ¨_, narrow_binop_mul_26âŸ©,
    âŸ¨_, narrow_binop_and_26âŸ©,
    âŸ¨_, narrow_binop_or_26âŸ©,
    âŸ¨_, narrow_binop_xor_26âŸ©,
    âŸ¨_, narrow_binop_add_27âŸ©,
    âŸ¨_, narrow_binop_sub_27âŸ©,
    âŸ¨_, narrow_binop_mul_27âŸ©,
    âŸ¨_, narrow_binop_and_27âŸ©,
    âŸ¨_, narrow_binop_or_27âŸ©,
    âŸ¨_, narrow_binop_xor_27âŸ©,
    âŸ¨_, narrow_binop_add_28âŸ©,
    âŸ¨_, narrow_binop_sub_28âŸ©,
    âŸ¨_, narrow_binop_mul_28âŸ©,
    âŸ¨_, narrow_binop_and_28âŸ©,
    âŸ¨_, narrow_binop_or_28âŸ©,
    âŸ¨_, narrow_binop_xor_28âŸ©,
    âŸ¨_, narrow_binop_add_29âŸ©,
    âŸ¨_, narrow_binop_sub_29âŸ©,
    âŸ¨_, narrow_binop_mul_29âŸ©,
    âŸ¨_, narrow_binop_and_29âŸ©,
    âŸ¨_, narrow_binop_or_29âŸ©,
    âŸ¨_, narrow_binop_xor_29âŸ©,
    âŸ¨_, narrow_binop_add_30âŸ©,
    âŸ¨_, narrow_binop_sub_30âŸ©,
    âŸ¨_, narrow_binop_mul_30âŸ©,
    âŸ¨_, narrow_binop_and_30âŸ©,
    âŸ¨_, narrow_binop_or_30âŸ©,
    âŸ¨_, narrow_binop_xor_30âŸ©,
    âŸ¨_, narrow_binop_add_31âŸ©,
    âŸ¨_, narrow_binop_sub_31âŸ©,
    âŸ¨_, narrow_binop_mul_31âŸ©,
    âŸ¨_, narrow_binop_and_31âŸ©,
    âŸ¨_, narrow_binop_or_31âŸ©,
    âŸ¨_, narrow_binop_xor_31âŸ©,
    âŸ¨_, narrow_binop_add_32âŸ©,
    âŸ¨_, narrow_binop_sub_32âŸ©,
    âŸ¨_, narrow_binop_mul_32âŸ©,
    âŸ¨_, narrow_binop_and_32âŸ©,
    âŸ¨_, narrow_binop_or_32âŸ©,
    âŸ¨_, narrow_binop_xor_32âŸ©,
    âŸ¨_, narrow_binop_add_33âŸ©,
    âŸ¨_, narrow_binop_sub_33âŸ©,
    âŸ¨_, narrow_binop_mul_33âŸ©,
    âŸ¨_, narrow_binop_and_33âŸ©,
    âŸ¨_, narrow_binop_or_33âŸ©,
    âŸ¨_, narrow_binop_xor_33âŸ©,
    âŸ¨_, narrow_binop_add_34âŸ©,
    âŸ¨_, narrow_binop_sub_34âŸ©,
    âŸ¨_, narrow_binop_mul_34âŸ©,
    âŸ¨_, narrow_binop_and_34âŸ©,
    âŸ¨_, narrow_binop_or_34âŸ©,
    âŸ¨_, narrow_binop_xor_34âŸ©,
    âŸ¨_, narrow_binop_add_35âŸ©,
    âŸ¨_, narrow_binop_sub_35âŸ©,
    âŸ¨_, narrow_binop_mul_35âŸ©,
    âŸ¨_, narrow_binop_and_35âŸ©,
    âŸ¨_, narrow_binop_or_35âŸ©,
    âŸ¨_, narrow_binop_xor_35âŸ©,
    âŸ¨_, narrow_binop_add_36âŸ©,
    âŸ¨_, narrow_binop_sub_36âŸ©,
    âŸ¨_, narrow_binop_mul_36âŸ©,
    âŸ¨_, narrow_binop_and_36âŸ©,
    âŸ¨_, narrow_binop_or_36âŸ©,
    âŸ¨_, narrow_binop_xor_36âŸ©,
    âŸ¨_, narrow_binop_add_37âŸ©,
    âŸ¨_, narrow_binop_sub_37âŸ©,
    âŸ¨_, narrow_binop_mul_37âŸ©,
    âŸ¨_, narrow_binop_and_37âŸ©,
    âŸ¨_, narrow_binop_or_37âŸ©,
    âŸ¨_, narrow_binop_xor_37âŸ©,
    âŸ¨_, narrow_binop_add_38âŸ©,
    âŸ¨_, narrow_binop_sub_38âŸ©,
    âŸ¨_, narrow_binop_mul_38âŸ©,
    âŸ¨_, narrow_binop_and_38âŸ©,
    âŸ¨_, narrow_binop_or_38âŸ©,
    âŸ¨_, narrow_binop_xor_38âŸ©,
    âŸ¨_, narrow_binop_add_39âŸ©,
    âŸ¨_, narrow_binop_sub_39âŸ©,
    âŸ¨_, narrow_binop_mul_39âŸ©,
    âŸ¨_, narrow_binop_and_39âŸ©,
    âŸ¨_, narrow_binop_or_39âŸ©,
    âŸ¨_, narrow_binop_xor_39âŸ©,
    âŸ¨_, narrow_binop_add_40âŸ©,
    âŸ¨_, narrow_binop_sub_40âŸ©,
    âŸ¨_, narrow_binop_mul_40âŸ©,
    âŸ¨_, narrow_binop_and_40âŸ©,
    âŸ¨_, narrow_binop_or_40âŸ©,
    âŸ¨_, narrow_binop_xor_40âŸ©,
    âŸ¨_, narrow_binop_add_41âŸ©,
    âŸ¨_, narrow_binop_sub_41âŸ©,
    âŸ¨_, narrow_binop_mul_41âŸ©,
    âŸ¨_, narrow_binop_and_41âŸ©,
    âŸ¨_, narrow_binop_or_41âŸ©,
    âŸ¨_, narrow_binop_xor_41âŸ©,
    âŸ¨_, narrow_binop_add_42âŸ©,
    âŸ¨_, narrow_binop_sub_42âŸ©,
    âŸ¨_, narrow_binop_mul_42âŸ©,
    âŸ¨_, narrow_binop_and_42âŸ©,
    âŸ¨_, narrow_binop_or_42âŸ©,
    âŸ¨_, narrow_binop_xor_42âŸ©,
    âŸ¨_, narrow_binop_add_43âŸ©,
    âŸ¨_, narrow_binop_sub_43âŸ©,
    âŸ¨_, narrow_binop_mul_43âŸ©,
    âŸ¨_, narrow_binop_and_43âŸ©,
    âŸ¨_, narrow_binop_or_43âŸ©,
    âŸ¨_, narrow_binop_xor_43âŸ©,
    âŸ¨_, narrow_binop_add_44âŸ©,
    âŸ¨_, narrow_binop_sub_44âŸ©,
    âŸ¨_, narrow_binop_mul_44âŸ©,
    âŸ¨_, narrow_binop_and_44âŸ©,
    âŸ¨_, narrow_binop_or_44âŸ©,
    âŸ¨_, narrow_binop_xor_44âŸ©,
    âŸ¨_, narrow_binop_add_45âŸ©,
    âŸ¨_, narrow_binop_sub_45âŸ©,
    âŸ¨_, narrow_binop_mul_45âŸ©,
    âŸ¨_, narrow_binop_and_45âŸ©,
    âŸ¨_, narrow_binop_or_45âŸ©,
    âŸ¨_, narrow_binop_xor_45âŸ©,
    âŸ¨_, narrow_binop_add_46âŸ©,
    âŸ¨_, narrow_binop_sub_46âŸ©,
    âŸ¨_, narrow_binop_mul_46âŸ©,
    âŸ¨_, narrow_binop_and_46âŸ©,
    âŸ¨_, narrow_binop_or_46âŸ©,
    âŸ¨_, narrow_binop_xor_46âŸ©,
    âŸ¨_, narrow_binop_add_47âŸ©,
    âŸ¨_, narrow_binop_sub_47âŸ©,
    âŸ¨_, narrow_binop_mul_47âŸ©,
    âŸ¨_, narrow_binop_and_47âŸ©,
    âŸ¨_, narrow_binop_or_47âŸ©,
    âŸ¨_, narrow_binop_xor_47âŸ©,
    âŸ¨_, narrow_binop_add_48âŸ©,
    âŸ¨_, narrow_binop_sub_48âŸ©,
    âŸ¨_, narrow_binop_mul_48âŸ©,
    âŸ¨_, narrow_binop_and_48âŸ©,
    âŸ¨_, narrow_binop_or_48âŸ©,
    âŸ¨_, narrow_binop_xor_48âŸ©,
    âŸ¨_, narrow_binop_add_49âŸ©,
    âŸ¨_, narrow_binop_sub_49âŸ©,
    âŸ¨_, narrow_binop_mul_49âŸ©,
    âŸ¨_, narrow_binop_and_49âŸ©,
    âŸ¨_, narrow_binop_or_49âŸ©,
    âŸ¨_, narrow_binop_xor_49âŸ©,
    âŸ¨_, narrow_binop_add_50âŸ©,
    âŸ¨_, narrow_binop_sub_50âŸ©,
    âŸ¨_, narrow_binop_mul_50âŸ©,
    âŸ¨_, narrow_binop_and_50âŸ©,
    âŸ¨_, narrow_binop_or_50âŸ©,
    âŸ¨_, narrow_binop_xor_50âŸ©
  ]
