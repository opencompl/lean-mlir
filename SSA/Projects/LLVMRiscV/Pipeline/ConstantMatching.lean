-- AUTOGENERATED Lean file

import SSA.Projects.LLVMRiscV.PeepholeRefine
import SSA.Projects.LLVMRiscV.Simpproc
import SSA.Projects.RISCV64.Tactic.SimpRiscV
import SSA.Projects.LLVMRiscV.Pipeline.mkRewrite

open LLVMRiscV


/-! ### sub_to_add -/

/--
Test the rewrite:
  (sub x, C) → (add x, -C)
-/
def sub_to_add_neg5 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg4 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg3 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_3 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_4 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add_5 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.sub %x, %c : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.add %x, %c : i64
      llvm.return %1 : i64
  }]

def sub_to_add : List (Σ Γ, LLVMPeepholeRewriteRefine 64 Γ) :=
  [
    ⟨_, sub_to_add_neg5⟩,
    ⟨_, sub_to_add_neg4⟩,
    ⟨_, sub_to_add_neg3⟩,
    ⟨_, sub_to_add_neg2⟩,
    ⟨_, sub_to_add_neg1⟩,
    ⟨_, sub_to_add_0⟩,
    ⟨_, sub_to_add_1⟩,
    ⟨_, sub_to_add_2⟩,
    ⟨_, sub_to_add_3⟩,
    ⟨_, sub_to_add_4⟩,
    ⟨_, sub_to_add_5⟩
  ]

/-! ### mul_to_shl -/

/--
Test the rewrite:
    (x * 2^n) → (x << n)
-/
def mul_to_shl_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (0) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_4 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_8 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (3) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_16 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (16) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_32 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (32) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (5) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_64 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (64) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (6) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_128 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (128) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (7) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_256 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (256) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl_512 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (512) : i64
      %0 = llvm.mul %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (9) : i64
      %0 = llvm.shl %x, %c : i64
      llvm.return %0 : i64
  }]

def mul_to_shl : List (Σ Γ, LLVMPeepholeRewriteRefine 64 Γ) :=
  [
    ⟨_, mul_to_shl_1⟩,
    ⟨_, mul_to_shl_2⟩,
    ⟨_, mul_to_shl_4⟩,
    ⟨_, mul_to_shl_8⟩,
    ⟨_, mul_to_shl_16⟩,
    ⟨_, mul_to_shl_32⟩,
    ⟨_, mul_to_shl_64⟩,
    ⟨_, mul_to_shl_128⟩,
    ⟨_, mul_to_shl_256⟩,
    ⟨_, mul_to_shl_512⟩
  ]

/-! ### urem_pow2_to_mask -/

/--
Test the rewrite:
    urem(x, 2^n) -> x & (2^n - 1)
-/
def urem_pow2_to_mask_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (0) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_4 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (3) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_8 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (7) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_16 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (16) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (15) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_32 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (32) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (31) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_64 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (64) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (63) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_128 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (128) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (127) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_256 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (256) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (255) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask_512 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (512) : i64
      %0 = llvm.urem %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (511) : i64
      %0 = llvm.and %x, %c : i64
      llvm.return %0 : i64
  }]

def urem_pow2_to_mask : List (Σ Γ, LLVMPeepholeRewriteRefine 64 Γ) :=
  [
    ⟨_, urem_pow2_to_mask_1⟩,
    ⟨_, urem_pow2_to_mask_2⟩,
    ⟨_, urem_pow2_to_mask_4⟩,
    ⟨_, urem_pow2_to_mask_8⟩,
    ⟨_, urem_pow2_to_mask_16⟩,
    ⟨_, urem_pow2_to_mask_32⟩,
    ⟨_, urem_pow2_to_mask_64⟩,
    ⟨_, urem_pow2_to_mask_128⟩,
    ⟨_, urem_pow2_to_mask_256⟩,
    ⟨_, urem_pow2_to_mask_512⟩
  ]

/-! ### canonicalize_icmp -/

/--
Test the rewrite:
  (cmp C, x) → (cmp x, C)
-/
def canonicalize_icmp_eq_neg5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.eq %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.eq %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ne_neg5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.ne %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.ne %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_uge_neg5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.uge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.ule %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ugt_neg5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.ugt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.ult %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ult_neg5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.ult %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.ugt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ule_neg5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.ule %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.uge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sgt_neg5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.sgt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.slt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sge_neg5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.sge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.sle %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_slt_neg5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.slt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.sgt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sle_neg5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.sle %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %1 = llvm.icmp.sge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_eq_neg4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.eq %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.eq %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ne_neg4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.ne %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.ne %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_uge_neg4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.uge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.ule %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ugt_neg4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.ugt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.ult %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ult_neg4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.ult %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.ugt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ule_neg4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.ule %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.uge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sgt_neg4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.sgt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.slt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sge_neg4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.sge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.sle %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_slt_neg4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.slt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.sgt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sle_neg4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.sle %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %1 = llvm.icmp.sge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_eq_neg3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.eq %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.eq %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ne_neg3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.ne %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.ne %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_uge_neg3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.uge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.ule %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ugt_neg3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.ugt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.ult %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ult_neg3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.ult %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.ugt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ule_neg3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.ule %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.uge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sgt_neg3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.sgt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.slt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sge_neg3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.sge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.sle %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_slt_neg3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.slt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.sgt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sle_neg3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.sle %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %1 = llvm.icmp.sge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_eq_neg2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.eq %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.eq %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ne_neg2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.ne %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.ne %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_uge_neg2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.uge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.ule %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ugt_neg2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.ugt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.ult %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ult_neg2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.ult %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.ugt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ule_neg2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.ule %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.uge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sgt_neg2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.sgt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.slt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sge_neg2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.sge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.sle %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_slt_neg2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.slt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.sgt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sle_neg2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.sle %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %1 = llvm.icmp.sge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_eq_neg1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.eq %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.eq %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ne_neg1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.ne %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.ne %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_uge_neg1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.uge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.ule %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ugt_neg1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.ugt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.ult %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ult_neg1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.ult %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.ugt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ule_neg1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.ule %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.uge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sgt_neg1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.sgt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.slt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sge_neg1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.sge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.sle %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_slt_neg1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.slt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.sgt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sle_neg1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.sle %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %1 = llvm.icmp.sge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_eq_0 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.eq %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.eq %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ne_0 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.ne %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.ne %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_uge_0 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.uge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.ule %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ugt_0 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.ugt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.ult %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ult_0 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.ult %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.ugt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ule_0 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.ule %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.uge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sgt_0 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.sgt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.slt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sge_0 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.sge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.sle %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_slt_0 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.slt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.sgt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sle_0 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.sle %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %1 = llvm.icmp.sge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_eq_1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.eq %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.eq %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ne_1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.ne %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.ne %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_uge_1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.uge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.ule %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ugt_1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.ugt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.ult %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ult_1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.ult %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.ugt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ule_1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.ule %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.uge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sgt_1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.sgt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.slt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sge_1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.sge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.sle %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_slt_1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.slt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.sgt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sle_1 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.sle %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %1 = llvm.icmp.sge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_eq_2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.eq %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.eq %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ne_2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.ne %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.ne %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_uge_2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.uge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.ule %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ugt_2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.ugt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.ult %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ult_2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.ult %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.ugt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ule_2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.ule %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.uge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sgt_2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.sgt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.slt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sge_2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.sge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.sle %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_slt_2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.slt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.sgt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sle_2 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.sle %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %1 = llvm.icmp.sge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_eq_3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.eq %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.eq %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ne_3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.ne %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.ne %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_uge_3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.uge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.ule %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ugt_3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.ugt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.ult %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ult_3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.ult %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.ugt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ule_3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.ule %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.uge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sgt_3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.sgt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.slt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sge_3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.sge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.sle %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_slt_3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.slt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.sgt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sle_3 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.sle %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %1 = llvm.icmp.sge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_eq_4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.eq %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.eq %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ne_4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.ne %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.ne %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_uge_4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.uge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.ule %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ugt_4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.ugt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.ult %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ult_4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.ult %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.ugt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ule_4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.ule %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.uge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sgt_4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.sgt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.slt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sge_4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.sge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.sle %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_slt_4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.slt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.sgt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sle_4 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.sle %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %1 = llvm.icmp.sge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_eq_5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.eq %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.eq %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ne_5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.ne %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.ne %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_uge_5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.uge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.ule %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ugt_5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.ugt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.ult %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ult_5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.ult %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.ugt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_ule_5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.ule %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.uge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sgt_5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.sgt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.slt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sge_5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.sge %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.sle %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_slt_5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.slt %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.sgt %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp_sle_5 : LLVMPeepholeRewriteRefine 1 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.sle %c, %x : i64
      llvm.return %1 : i1
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %1 = llvm.icmp.sge %x, %c : i64
      llvm.return %1 : i1
  }]

def canonicalize_icmp : List (Σ Γ, LLVMPeepholeRewriteRefine 1 Γ) :=
  [
    ⟨_, canonicalize_icmp_eq_neg5⟩,
    ⟨_, canonicalize_icmp_ne_neg5⟩,
    ⟨_, canonicalize_icmp_uge_neg5⟩,
    ⟨_, canonicalize_icmp_ugt_neg5⟩,
    ⟨_, canonicalize_icmp_ult_neg5⟩,
    ⟨_, canonicalize_icmp_ule_neg5⟩,
    ⟨_, canonicalize_icmp_sgt_neg5⟩,
    ⟨_, canonicalize_icmp_sge_neg5⟩,
    ⟨_, canonicalize_icmp_slt_neg5⟩,
    ⟨_, canonicalize_icmp_sle_neg5⟩,
    ⟨_, canonicalize_icmp_eq_neg4⟩,
    ⟨_, canonicalize_icmp_ne_neg4⟩,
    ⟨_, canonicalize_icmp_uge_neg4⟩,
    ⟨_, canonicalize_icmp_ugt_neg4⟩,
    ⟨_, canonicalize_icmp_ult_neg4⟩,
    ⟨_, canonicalize_icmp_ule_neg4⟩,
    ⟨_, canonicalize_icmp_sgt_neg4⟩,
    ⟨_, canonicalize_icmp_sge_neg4⟩,
    ⟨_, canonicalize_icmp_slt_neg4⟩,
    ⟨_, canonicalize_icmp_sle_neg4⟩,
    ⟨_, canonicalize_icmp_eq_neg3⟩,
    ⟨_, canonicalize_icmp_ne_neg3⟩,
    ⟨_, canonicalize_icmp_uge_neg3⟩,
    ⟨_, canonicalize_icmp_ugt_neg3⟩,
    ⟨_, canonicalize_icmp_ult_neg3⟩,
    ⟨_, canonicalize_icmp_ule_neg3⟩,
    ⟨_, canonicalize_icmp_sgt_neg3⟩,
    ⟨_, canonicalize_icmp_sge_neg3⟩,
    ⟨_, canonicalize_icmp_slt_neg3⟩,
    ⟨_, canonicalize_icmp_sle_neg3⟩,
    ⟨_, canonicalize_icmp_eq_neg2⟩,
    ⟨_, canonicalize_icmp_ne_neg2⟩,
    ⟨_, canonicalize_icmp_uge_neg2⟩,
    ⟨_, canonicalize_icmp_ugt_neg2⟩,
    ⟨_, canonicalize_icmp_ult_neg2⟩,
    ⟨_, canonicalize_icmp_ule_neg2⟩,
    ⟨_, canonicalize_icmp_sgt_neg2⟩,
    ⟨_, canonicalize_icmp_sge_neg2⟩,
    ⟨_, canonicalize_icmp_slt_neg2⟩,
    ⟨_, canonicalize_icmp_sle_neg2⟩,
    ⟨_, canonicalize_icmp_eq_neg1⟩,
    ⟨_, canonicalize_icmp_ne_neg1⟩,
    ⟨_, canonicalize_icmp_uge_neg1⟩,
    ⟨_, canonicalize_icmp_ugt_neg1⟩,
    ⟨_, canonicalize_icmp_ult_neg1⟩,
    ⟨_, canonicalize_icmp_ule_neg1⟩,
    ⟨_, canonicalize_icmp_sgt_neg1⟩,
    ⟨_, canonicalize_icmp_sge_neg1⟩,
    ⟨_, canonicalize_icmp_slt_neg1⟩,
    ⟨_, canonicalize_icmp_sle_neg1⟩,
    ⟨_, canonicalize_icmp_eq_0⟩,
    ⟨_, canonicalize_icmp_ne_0⟩,
    ⟨_, canonicalize_icmp_uge_0⟩,
    ⟨_, canonicalize_icmp_ugt_0⟩,
    ⟨_, canonicalize_icmp_ult_0⟩,
    ⟨_, canonicalize_icmp_ule_0⟩,
    ⟨_, canonicalize_icmp_sgt_0⟩,
    ⟨_, canonicalize_icmp_sge_0⟩,
    ⟨_, canonicalize_icmp_slt_0⟩,
    ⟨_, canonicalize_icmp_sle_0⟩,
    ⟨_, canonicalize_icmp_eq_1⟩,
    ⟨_, canonicalize_icmp_ne_1⟩,
    ⟨_, canonicalize_icmp_uge_1⟩,
    ⟨_, canonicalize_icmp_ugt_1⟩,
    ⟨_, canonicalize_icmp_ult_1⟩,
    ⟨_, canonicalize_icmp_ule_1⟩,
    ⟨_, canonicalize_icmp_sgt_1⟩,
    ⟨_, canonicalize_icmp_sge_1⟩,
    ⟨_, canonicalize_icmp_slt_1⟩,
    ⟨_, canonicalize_icmp_sle_1⟩,
    ⟨_, canonicalize_icmp_eq_2⟩,
    ⟨_, canonicalize_icmp_ne_2⟩,
    ⟨_, canonicalize_icmp_uge_2⟩,
    ⟨_, canonicalize_icmp_ugt_2⟩,
    ⟨_, canonicalize_icmp_ult_2⟩,
    ⟨_, canonicalize_icmp_ule_2⟩,
    ⟨_, canonicalize_icmp_sgt_2⟩,
    ⟨_, canonicalize_icmp_sge_2⟩,
    ⟨_, canonicalize_icmp_slt_2⟩,
    ⟨_, canonicalize_icmp_sle_2⟩,
    ⟨_, canonicalize_icmp_eq_3⟩,
    ⟨_, canonicalize_icmp_ne_3⟩,
    ⟨_, canonicalize_icmp_uge_3⟩,
    ⟨_, canonicalize_icmp_ugt_3⟩,
    ⟨_, canonicalize_icmp_ult_3⟩,
    ⟨_, canonicalize_icmp_ule_3⟩,
    ⟨_, canonicalize_icmp_sgt_3⟩,
    ⟨_, canonicalize_icmp_sge_3⟩,
    ⟨_, canonicalize_icmp_slt_3⟩,
    ⟨_, canonicalize_icmp_sle_3⟩,
    ⟨_, canonicalize_icmp_eq_4⟩,
    ⟨_, canonicalize_icmp_ne_4⟩,
    ⟨_, canonicalize_icmp_uge_4⟩,
    ⟨_, canonicalize_icmp_ugt_4⟩,
    ⟨_, canonicalize_icmp_ult_4⟩,
    ⟨_, canonicalize_icmp_ule_4⟩,
    ⟨_, canonicalize_icmp_sgt_4⟩,
    ⟨_, canonicalize_icmp_sge_4⟩,
    ⟨_, canonicalize_icmp_slt_4⟩,
    ⟨_, canonicalize_icmp_sle_4⟩,
    ⟨_, canonicalize_icmp_eq_5⟩,
    ⟨_, canonicalize_icmp_ne_5⟩,
    ⟨_, canonicalize_icmp_uge_5⟩,
    ⟨_, canonicalize_icmp_ugt_5⟩,
    ⟨_, canonicalize_icmp_ult_5⟩,
    ⟨_, canonicalize_icmp_ule_5⟩,
    ⟨_, canonicalize_icmp_sgt_5⟩,
    ⟨_, canonicalize_icmp_sge_5⟩,
    ⟨_, canonicalize_icmp_slt_5⟩,
    ⟨_, canonicalize_icmp_sle_5⟩
  ]

/-! ### mulh_to_lshr -/

/--
Test the rewrite:
  (mulh x, n^2) → (sra x, (64-n))
-/
def mulh_to_lshr_2 : RISCVPeepholeRewrite [Ty.riscv (.bv)] where
  lhs := [LV| {
    ^entry (%x: !riscv.reg):
      %c = li (2) : !riscv.reg
      %0 = mulh %c, %x : !riscv.reg
      ret %0 : !riscv.reg
  }]
  rhs := [LV| {
    ^entry (%x: !riscv.reg):
      %c = li (63) : !riscv.reg
      %0 = sra %x, %c : !riscv.reg
      ret %0 : !riscv.reg
  }]

def mulh_to_lshr_4 : RISCVPeepholeRewrite [Ty.riscv (.bv)] where
  lhs := [LV| {
    ^entry (%x: !riscv.reg):
      %c = li (4) : !riscv.reg
      %0 = mulh %c, %x : !riscv.reg
      ret %0 : !riscv.reg
  }]
  rhs := [LV| {
    ^entry (%x: !riscv.reg):
      %c = li (62) : !riscv.reg
      %0 = sra %x, %c : !riscv.reg
      ret %0 : !riscv.reg
  }]

def mulh_to_lshr_8 : RISCVPeepholeRewrite [Ty.riscv (.bv)] where
  lhs := [LV| {
    ^entry (%x: !riscv.reg):
      %c = li (8) : !riscv.reg
      %0 = mulh %c, %x : !riscv.reg
      ret %0 : !riscv.reg
  }]
  rhs := [LV| {
    ^entry (%x: !riscv.reg):
      %c = li (61) : !riscv.reg
      %0 = sra %x, %c : !riscv.reg
      ret %0 : !riscv.reg
  }]

def mulh_to_lshr_16 : RISCVPeepholeRewrite [Ty.riscv (.bv)] where
  lhs := [LV| {
    ^entry (%x: !riscv.reg):
      %c = li (16) : !riscv.reg
      %0 = mulh %c, %x : !riscv.reg
      ret %0 : !riscv.reg
  }]
  rhs := [LV| {
    ^entry (%x: !riscv.reg):
      %c = li (60) : !riscv.reg
      %0 = sra %x, %c : !riscv.reg
      ret %0 : !riscv.reg
  }]

def mulh_to_lshr_32 : RISCVPeepholeRewrite [Ty.riscv (.bv)] where
  lhs := [LV| {
    ^entry (%x: !riscv.reg):
      %c = li (32) : !riscv.reg
      %0 = mulh %c, %x : !riscv.reg
      ret %0 : !riscv.reg
  }]
  rhs := [LV| {
    ^entry (%x: !riscv.reg):
      %c = li (59) : !riscv.reg
      %0 = sra %x, %c : !riscv.reg
      ret %0 : !riscv.reg
  }]

def mulh_to_lshr_64 : RISCVPeepholeRewrite [Ty.riscv (.bv)] where
  lhs := [LV| {
    ^entry (%x: !riscv.reg):
      %c = li (64) : !riscv.reg
      %0 = mulh %c, %x : !riscv.reg
      ret %0 : !riscv.reg
  }]
  rhs := [LV| {
    ^entry (%x: !riscv.reg):
      %c = li (58) : !riscv.reg
      %0 = sra %x, %c : !riscv.reg
      ret %0 : !riscv.reg
  }]

def mulh_to_lshr_128 : RISCVPeepholeRewrite [Ty.riscv (.bv)] where
  lhs := [LV| {
    ^entry (%x: !riscv.reg):
      %c = li (128) : !riscv.reg
      %0 = mulh %c, %x : !riscv.reg
      ret %0 : !riscv.reg
  }]
  rhs := [LV| {
    ^entry (%x: !riscv.reg):
      %c = li (57) : !riscv.reg
      %0 = sra %x, %c : !riscv.reg
      ret %0 : !riscv.reg
  }]

def mulh_to_lshr_256 : RISCVPeepholeRewrite [Ty.riscv (.bv)] where
  lhs := [LV| {
    ^entry (%x: !riscv.reg):
      %c = li (256) : !riscv.reg
      %0 = mulh %c, %x : !riscv.reg
      ret %0 : !riscv.reg
  }]
  rhs := [LV| {
    ^entry (%x: !riscv.reg):
      %c = li (56) : !riscv.reg
      %0 = sra %x, %c : !riscv.reg
      ret %0 : !riscv.reg
  }]

def mulh_to_lshr_512 : RISCVPeepholeRewrite [Ty.riscv (.bv)] where
  lhs := [LV| {
    ^entry (%x: !riscv.reg):
      %c = li (512) : !riscv.reg
      %0 = mulh %c, %x : !riscv.reg
      ret %0 : !riscv.reg
  }]
  rhs := [LV| {
    ^entry (%x: !riscv.reg):
      %c = li (55) : !riscv.reg
      %0 = sra %x, %c : !riscv.reg
      ret %0 : !riscv.reg
  }]

def mulh_to_lshr : List (Σ Γ, RISCVPeepholeRewrite Γ) :=
  [
    ⟨_, mulh_to_lshr_2⟩,
    ⟨_, mulh_to_lshr_4⟩,
    ⟨_, mulh_to_lshr_8⟩,
    ⟨_, mulh_to_lshr_16⟩,
    ⟨_, mulh_to_lshr_32⟩,
    ⟨_, mulh_to_lshr_64⟩,
    ⟨_, mulh_to_lshr_128⟩,
    ⟨_, mulh_to_lshr_256⟩,
    ⟨_, mulh_to_lshr_512⟩
  ]



/-! ### cast_combines_narrow_binops -/

/--
Test the rewrite:
    trunc (binop X, C) --> binop (trunc X, trunc C)
-/
def narrow_binop_add_neg50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_neg1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_neg1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_neg1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_neg1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_neg1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_neg1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant -1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_0 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_0 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_0 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_0 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_0 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_0 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 0 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_1 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 1 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_2 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 2 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_3 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 3 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_4 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 4 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_5 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 5 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_6 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 6 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_7 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 7 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_8 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 8 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_9 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 9 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_10 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 10 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_11 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 11 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_12 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 12 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_13 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 13 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_14 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 14 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_15 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 15 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_16 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 16 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_17 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 17 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_18 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 18 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_19 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 19 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_20 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 20 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_21 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 21 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_22 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 22 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_23 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 23 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_24 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 24 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_25 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 25 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_26 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 26 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_27 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 27 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_28 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 28 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_29 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 29 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_30 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 30 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_31 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 31 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_32 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 32 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_33 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 33 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_34 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 34 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_35 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 35 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_36 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 36 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_37 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 37 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_38 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 38 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_39 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 39 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_40 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 40 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_41 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 41 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_42 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 42 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_43 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 43 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_44 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 44 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_45 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 45 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_46 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 46 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_47 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 47 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_48 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 48 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_49 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 49 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_add_50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.add %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.add %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_sub_50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.sub %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.sub %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_mul_50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.mul %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.mul %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_and_50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.and %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.and %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_or_50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.or %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.or %0, %1 : i32
      llvm.return %2 : i32
  }]

def narrow_binop_xor_50 : LLVMPeepholeRewriteRefine 32 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.xor %x, %c : i64
      %1 = llvm.trunc %0 : i64 to i32
      llvm.return %1 : i32
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant 50 : i64
      %0 = llvm.trunc %x : i64 to i32
      %1 = llvm.trunc %c : i64 to i32
      %2 = llvm.xor %0, %1 : i32
      llvm.return %2 : i32
  }]

def cast_combines_narrow_binops : List (Σ Γ, LLVMPeepholeRewriteRefine 64 Γ) :=
  [
    ⟨_, narrow_binop_add_neg50⟩,
    ⟨_, narrow_binop_sub_neg50⟩,
    ⟨_, narrow_binop_mul_neg50⟩,
    ⟨_, narrow_binop_and_neg50⟩,
    ⟨_, narrow_binop_or_neg50⟩,
    ⟨_, narrow_binop_xor_neg50⟩,
    ⟨_, narrow_binop_add_neg49⟩,
    ⟨_, narrow_binop_sub_neg49⟩,
    ⟨_, narrow_binop_mul_neg49⟩,
    ⟨_, narrow_binop_and_neg49⟩,
    ⟨_, narrow_binop_or_neg49⟩,
    ⟨_, narrow_binop_xor_neg49⟩,
    ⟨_, narrow_binop_add_neg48⟩,
    ⟨_, narrow_binop_sub_neg48⟩,
    ⟨_, narrow_binop_mul_neg48⟩,
    ⟨_, narrow_binop_and_neg48⟩,
    ⟨_, narrow_binop_or_neg48⟩,
    ⟨_, narrow_binop_xor_neg48⟩,
    ⟨_, narrow_binop_add_neg47⟩,
    ⟨_, narrow_binop_sub_neg47⟩,
    ⟨_, narrow_binop_mul_neg47⟩,
    ⟨_, narrow_binop_and_neg47⟩,
    ⟨_, narrow_binop_or_neg47⟩,
    ⟨_, narrow_binop_xor_neg47⟩,
    ⟨_, narrow_binop_add_neg46⟩,
    ⟨_, narrow_binop_sub_neg46⟩,
    ⟨_, narrow_binop_mul_neg46⟩,
    ⟨_, narrow_binop_and_neg46⟩,
    ⟨_, narrow_binop_or_neg46⟩,
    ⟨_, narrow_binop_xor_neg46⟩,
    ⟨_, narrow_binop_add_neg45⟩,
    ⟨_, narrow_binop_sub_neg45⟩,
    ⟨_, narrow_binop_mul_neg45⟩,
    ⟨_, narrow_binop_and_neg45⟩,
    ⟨_, narrow_binop_or_neg45⟩,
    ⟨_, narrow_binop_xor_neg45⟩,
    ⟨_, narrow_binop_add_neg44⟩,
    ⟨_, narrow_binop_sub_neg44⟩,
    ⟨_, narrow_binop_mul_neg44⟩,
    ⟨_, narrow_binop_and_neg44⟩,
    ⟨_, narrow_binop_or_neg44⟩,
    ⟨_, narrow_binop_xor_neg44⟩,
    ⟨_, narrow_binop_add_neg43⟩,
    ⟨_, narrow_binop_sub_neg43⟩,
    ⟨_, narrow_binop_mul_neg43⟩,
    ⟨_, narrow_binop_and_neg43⟩,
    ⟨_, narrow_binop_or_neg43⟩,
    ⟨_, narrow_binop_xor_neg43⟩,
    ⟨_, narrow_binop_add_neg42⟩,
    ⟨_, narrow_binop_sub_neg42⟩,
    ⟨_, narrow_binop_mul_neg42⟩,
    ⟨_, narrow_binop_and_neg42⟩,
    ⟨_, narrow_binop_or_neg42⟩,
    ⟨_, narrow_binop_xor_neg42⟩,
    ⟨_, narrow_binop_add_neg41⟩,
    ⟨_, narrow_binop_sub_neg41⟩,
    ⟨_, narrow_binop_mul_neg41⟩,
    ⟨_, narrow_binop_and_neg41⟩,
    ⟨_, narrow_binop_or_neg41⟩,
    ⟨_, narrow_binop_xor_neg41⟩,
    ⟨_, narrow_binop_add_neg40⟩,
    ⟨_, narrow_binop_sub_neg40⟩,
    ⟨_, narrow_binop_mul_neg40⟩,
    ⟨_, narrow_binop_and_neg40⟩,
    ⟨_, narrow_binop_or_neg40⟩,
    ⟨_, narrow_binop_xor_neg40⟩,
    ⟨_, narrow_binop_add_neg39⟩,
    ⟨_, narrow_binop_sub_neg39⟩,
    ⟨_, narrow_binop_mul_neg39⟩,
    ⟨_, narrow_binop_and_neg39⟩,
    ⟨_, narrow_binop_or_neg39⟩,
    ⟨_, narrow_binop_xor_neg39⟩,
    ⟨_, narrow_binop_add_neg38⟩,
    ⟨_, narrow_binop_sub_neg38⟩,
    ⟨_, narrow_binop_mul_neg38⟩,
    ⟨_, narrow_binop_and_neg38⟩,
    ⟨_, narrow_binop_or_neg38⟩,
    ⟨_, narrow_binop_xor_neg38⟩,
    ⟨_, narrow_binop_add_neg37⟩,
    ⟨_, narrow_binop_sub_neg37⟩,
    ⟨_, narrow_binop_mul_neg37⟩,
    ⟨_, narrow_binop_and_neg37⟩,
    ⟨_, narrow_binop_or_neg37⟩,
    ⟨_, narrow_binop_xor_neg37⟩,
    ⟨_, narrow_binop_add_neg36⟩,
    ⟨_, narrow_binop_sub_neg36⟩,
    ⟨_, narrow_binop_mul_neg36⟩,
    ⟨_, narrow_binop_and_neg36⟩,
    ⟨_, narrow_binop_or_neg36⟩,
    ⟨_, narrow_binop_xor_neg36⟩,
    ⟨_, narrow_binop_add_neg35⟩,
    ⟨_, narrow_binop_sub_neg35⟩,
    ⟨_, narrow_binop_mul_neg35⟩,
    ⟨_, narrow_binop_and_neg35⟩,
    ⟨_, narrow_binop_or_neg35⟩,
    ⟨_, narrow_binop_xor_neg35⟩,
    ⟨_, narrow_binop_add_neg34⟩,
    ⟨_, narrow_binop_sub_neg34⟩,
    ⟨_, narrow_binop_mul_neg34⟩,
    ⟨_, narrow_binop_and_neg34⟩,
    ⟨_, narrow_binop_or_neg34⟩,
    ⟨_, narrow_binop_xor_neg34⟩,
    ⟨_, narrow_binop_add_neg33⟩,
    ⟨_, narrow_binop_sub_neg33⟩,
    ⟨_, narrow_binop_mul_neg33⟩,
    ⟨_, narrow_binop_and_neg33⟩,
    ⟨_, narrow_binop_or_neg33⟩,
    ⟨_, narrow_binop_xor_neg33⟩,
    ⟨_, narrow_binop_add_neg32⟩,
    ⟨_, narrow_binop_sub_neg32⟩,
    ⟨_, narrow_binop_mul_neg32⟩,
    ⟨_, narrow_binop_and_neg32⟩,
    ⟨_, narrow_binop_or_neg32⟩,
    ⟨_, narrow_binop_xor_neg32⟩,
    ⟨_, narrow_binop_add_neg31⟩,
    ⟨_, narrow_binop_sub_neg31⟩,
    ⟨_, narrow_binop_mul_neg31⟩,
    ⟨_, narrow_binop_and_neg31⟩,
    ⟨_, narrow_binop_or_neg31⟩,
    ⟨_, narrow_binop_xor_neg31⟩,
    ⟨_, narrow_binop_add_neg30⟩,
    ⟨_, narrow_binop_sub_neg30⟩,
    ⟨_, narrow_binop_mul_neg30⟩,
    ⟨_, narrow_binop_and_neg30⟩,
    ⟨_, narrow_binop_or_neg30⟩,
    ⟨_, narrow_binop_xor_neg30⟩,
    ⟨_, narrow_binop_add_neg29⟩,
    ⟨_, narrow_binop_sub_neg29⟩,
    ⟨_, narrow_binop_mul_neg29⟩,
    ⟨_, narrow_binop_and_neg29⟩,
    ⟨_, narrow_binop_or_neg29⟩,
    ⟨_, narrow_binop_xor_neg29⟩,
    ⟨_, narrow_binop_add_neg28⟩,
    ⟨_, narrow_binop_sub_neg28⟩,
    ⟨_, narrow_binop_mul_neg28⟩,
    ⟨_, narrow_binop_and_neg28⟩,
    ⟨_, narrow_binop_or_neg28⟩,
    ⟨_, narrow_binop_xor_neg28⟩,
    ⟨_, narrow_binop_add_neg27⟩,
    ⟨_, narrow_binop_sub_neg27⟩,
    ⟨_, narrow_binop_mul_neg27⟩,
    ⟨_, narrow_binop_and_neg27⟩,
    ⟨_, narrow_binop_or_neg27⟩,
    ⟨_, narrow_binop_xor_neg27⟩,
    ⟨_, narrow_binop_add_neg26⟩,
    ⟨_, narrow_binop_sub_neg26⟩,
    ⟨_, narrow_binop_mul_neg26⟩,
    ⟨_, narrow_binop_and_neg26⟩,
    ⟨_, narrow_binop_or_neg26⟩,
    ⟨_, narrow_binop_xor_neg26⟩,
    ⟨_, narrow_binop_add_neg25⟩,
    ⟨_, narrow_binop_sub_neg25⟩,
    ⟨_, narrow_binop_mul_neg25⟩,
    ⟨_, narrow_binop_and_neg25⟩,
    ⟨_, narrow_binop_or_neg25⟩,
    ⟨_, narrow_binop_xor_neg25⟩,
    ⟨_, narrow_binop_add_neg24⟩,
    ⟨_, narrow_binop_sub_neg24⟩,
    ⟨_, narrow_binop_mul_neg24⟩,
    ⟨_, narrow_binop_and_neg24⟩,
    ⟨_, narrow_binop_or_neg24⟩,
    ⟨_, narrow_binop_xor_neg24⟩,
    ⟨_, narrow_binop_add_neg23⟩,
    ⟨_, narrow_binop_sub_neg23⟩,
    ⟨_, narrow_binop_mul_neg23⟩,
    ⟨_, narrow_binop_and_neg23⟩,
    ⟨_, narrow_binop_or_neg23⟩,
    ⟨_, narrow_binop_xor_neg23⟩,
    ⟨_, narrow_binop_add_neg22⟩,
    ⟨_, narrow_binop_sub_neg22⟩,
    ⟨_, narrow_binop_mul_neg22⟩,
    ⟨_, narrow_binop_and_neg22⟩,
    ⟨_, narrow_binop_or_neg22⟩,
    ⟨_, narrow_binop_xor_neg22⟩,
    ⟨_, narrow_binop_add_neg21⟩,
    ⟨_, narrow_binop_sub_neg21⟩,
    ⟨_, narrow_binop_mul_neg21⟩,
    ⟨_, narrow_binop_and_neg21⟩,
    ⟨_, narrow_binop_or_neg21⟩,
    ⟨_, narrow_binop_xor_neg21⟩,
    ⟨_, narrow_binop_add_neg20⟩,
    ⟨_, narrow_binop_sub_neg20⟩,
    ⟨_, narrow_binop_mul_neg20⟩,
    ⟨_, narrow_binop_and_neg20⟩,
    ⟨_, narrow_binop_or_neg20⟩,
    ⟨_, narrow_binop_xor_neg20⟩,
    ⟨_, narrow_binop_add_neg19⟩,
    ⟨_, narrow_binop_sub_neg19⟩,
    ⟨_, narrow_binop_mul_neg19⟩,
    ⟨_, narrow_binop_and_neg19⟩,
    ⟨_, narrow_binop_or_neg19⟩,
    ⟨_, narrow_binop_xor_neg19⟩,
    ⟨_, narrow_binop_add_neg18⟩,
    ⟨_, narrow_binop_sub_neg18⟩,
    ⟨_, narrow_binop_mul_neg18⟩,
    ⟨_, narrow_binop_and_neg18⟩,
    ⟨_, narrow_binop_or_neg18⟩,
    ⟨_, narrow_binop_xor_neg18⟩,
    ⟨_, narrow_binop_add_neg17⟩,
    ⟨_, narrow_binop_sub_neg17⟩,
    ⟨_, narrow_binop_mul_neg17⟩,
    ⟨_, narrow_binop_and_neg17⟩,
    ⟨_, narrow_binop_or_neg17⟩,
    ⟨_, narrow_binop_xor_neg17⟩,
    ⟨_, narrow_binop_add_neg16⟩,
    ⟨_, narrow_binop_sub_neg16⟩,
    ⟨_, narrow_binop_mul_neg16⟩,
    ⟨_, narrow_binop_and_neg16⟩,
    ⟨_, narrow_binop_or_neg16⟩,
    ⟨_, narrow_binop_xor_neg16⟩,
    ⟨_, narrow_binop_add_neg15⟩,
    ⟨_, narrow_binop_sub_neg15⟩,
    ⟨_, narrow_binop_mul_neg15⟩,
    ⟨_, narrow_binop_and_neg15⟩,
    ⟨_, narrow_binop_or_neg15⟩,
    ⟨_, narrow_binop_xor_neg15⟩,
    ⟨_, narrow_binop_add_neg14⟩,
    ⟨_, narrow_binop_sub_neg14⟩,
    ⟨_, narrow_binop_mul_neg14⟩,
    ⟨_, narrow_binop_and_neg14⟩,
    ⟨_, narrow_binop_or_neg14⟩,
    ⟨_, narrow_binop_xor_neg14⟩,
    ⟨_, narrow_binop_add_neg13⟩,
    ⟨_, narrow_binop_sub_neg13⟩,
    ⟨_, narrow_binop_mul_neg13⟩,
    ⟨_, narrow_binop_and_neg13⟩,
    ⟨_, narrow_binop_or_neg13⟩,
    ⟨_, narrow_binop_xor_neg13⟩,
    ⟨_, narrow_binop_add_neg12⟩,
    ⟨_, narrow_binop_sub_neg12⟩,
    ⟨_, narrow_binop_mul_neg12⟩,
    ⟨_, narrow_binop_and_neg12⟩,
    ⟨_, narrow_binop_or_neg12⟩,
    ⟨_, narrow_binop_xor_neg12⟩,
    ⟨_, narrow_binop_add_neg11⟩,
    ⟨_, narrow_binop_sub_neg11⟩,
    ⟨_, narrow_binop_mul_neg11⟩,
    ⟨_, narrow_binop_and_neg11⟩,
    ⟨_, narrow_binop_or_neg11⟩,
    ⟨_, narrow_binop_xor_neg11⟩,
    ⟨_, narrow_binop_add_neg10⟩,
    ⟨_, narrow_binop_sub_neg10⟩,
    ⟨_, narrow_binop_mul_neg10⟩,
    ⟨_, narrow_binop_and_neg10⟩,
    ⟨_, narrow_binop_or_neg10⟩,
    ⟨_, narrow_binop_xor_neg10⟩,
    ⟨_, narrow_binop_add_neg9⟩,
    ⟨_, narrow_binop_sub_neg9⟩,
    ⟨_, narrow_binop_mul_neg9⟩,
    ⟨_, narrow_binop_and_neg9⟩,
    ⟨_, narrow_binop_or_neg9⟩,
    ⟨_, narrow_binop_xor_neg9⟩,
    ⟨_, narrow_binop_add_neg8⟩,
    ⟨_, narrow_binop_sub_neg8⟩,
    ⟨_, narrow_binop_mul_neg8⟩,
    ⟨_, narrow_binop_and_neg8⟩,
    ⟨_, narrow_binop_or_neg8⟩,
    ⟨_, narrow_binop_xor_neg8⟩,
    ⟨_, narrow_binop_add_neg7⟩,
    ⟨_, narrow_binop_sub_neg7⟩,
    ⟨_, narrow_binop_mul_neg7⟩,
    ⟨_, narrow_binop_and_neg7⟩,
    ⟨_, narrow_binop_or_neg7⟩,
    ⟨_, narrow_binop_xor_neg7⟩,
    ⟨_, narrow_binop_add_neg6⟩,
    ⟨_, narrow_binop_sub_neg6⟩,
    ⟨_, narrow_binop_mul_neg6⟩,
    ⟨_, narrow_binop_and_neg6⟩,
    ⟨_, narrow_binop_or_neg6⟩,
    ⟨_, narrow_binop_xor_neg6⟩,
    ⟨_, narrow_binop_add_neg5⟩,
    ⟨_, narrow_binop_sub_neg5⟩,
    ⟨_, narrow_binop_mul_neg5⟩,
    ⟨_, narrow_binop_and_neg5⟩,
    ⟨_, narrow_binop_or_neg5⟩,
    ⟨_, narrow_binop_xor_neg5⟩,
    ⟨_, narrow_binop_add_neg4⟩,
    ⟨_, narrow_binop_sub_neg4⟩,
    ⟨_, narrow_binop_mul_neg4⟩,
    ⟨_, narrow_binop_and_neg4⟩,
    ⟨_, narrow_binop_or_neg4⟩,
    ⟨_, narrow_binop_xor_neg4⟩,
    ⟨_, narrow_binop_add_neg3⟩,
    ⟨_, narrow_binop_sub_neg3⟩,
    ⟨_, narrow_binop_mul_neg3⟩,
    ⟨_, narrow_binop_and_neg3⟩,
    ⟨_, narrow_binop_or_neg3⟩,
    ⟨_, narrow_binop_xor_neg3⟩,
    ⟨_, narrow_binop_add_neg2⟩,
    ⟨_, narrow_binop_sub_neg2⟩,
    ⟨_, narrow_binop_mul_neg2⟩,
    ⟨_, narrow_binop_and_neg2⟩,
    ⟨_, narrow_binop_or_neg2⟩,
    ⟨_, narrow_binop_xor_neg2⟩,
    ⟨_, narrow_binop_add_neg1⟩,
    ⟨_, narrow_binop_sub_neg1⟩,
    ⟨_, narrow_binop_mul_neg1⟩,
    ⟨_, narrow_binop_and_neg1⟩,
    ⟨_, narrow_binop_or_neg1⟩,
    ⟨_, narrow_binop_xor_neg1⟩,
    ⟨_, narrow_binop_add_0⟩,
    ⟨_, narrow_binop_sub_0⟩,
    ⟨_, narrow_binop_mul_0⟩,
    ⟨_, narrow_binop_and_0⟩,
    ⟨_, narrow_binop_or_0⟩,
    ⟨_, narrow_binop_xor_0⟩,
    ⟨_, narrow_binop_add_1⟩,
    ⟨_, narrow_binop_sub_1⟩,
    ⟨_, narrow_binop_mul_1⟩,
    ⟨_, narrow_binop_and_1⟩,
    ⟨_, narrow_binop_or_1⟩,
    ⟨_, narrow_binop_xor_1⟩,
    ⟨_, narrow_binop_add_2⟩,
    ⟨_, narrow_binop_sub_2⟩,
    ⟨_, narrow_binop_mul_2⟩,
    ⟨_, narrow_binop_and_2⟩,
    ⟨_, narrow_binop_or_2⟩,
    ⟨_, narrow_binop_xor_2⟩,
    ⟨_, narrow_binop_add_3⟩,
    ⟨_, narrow_binop_sub_3⟩,
    ⟨_, narrow_binop_mul_3⟩,
    ⟨_, narrow_binop_and_3⟩,
    ⟨_, narrow_binop_or_3⟩,
    ⟨_, narrow_binop_xor_3⟩,
    ⟨_, narrow_binop_add_4⟩,
    ⟨_, narrow_binop_sub_4⟩,
    ⟨_, narrow_binop_mul_4⟩,
    ⟨_, narrow_binop_and_4⟩,
    ⟨_, narrow_binop_or_4⟩,
    ⟨_, narrow_binop_xor_4⟩,
    ⟨_, narrow_binop_add_5⟩,
    ⟨_, narrow_binop_sub_5⟩,
    ⟨_, narrow_binop_mul_5⟩,
    ⟨_, narrow_binop_and_5⟩,
    ⟨_, narrow_binop_or_5⟩,
    ⟨_, narrow_binop_xor_5⟩,
    ⟨_, narrow_binop_add_6⟩,
    ⟨_, narrow_binop_sub_6⟩,
    ⟨_, narrow_binop_mul_6⟩,
    ⟨_, narrow_binop_and_6⟩,
    ⟨_, narrow_binop_or_6⟩,
    ⟨_, narrow_binop_xor_6⟩,
    ⟨_, narrow_binop_add_7⟩,
    ⟨_, narrow_binop_sub_7⟩,
    ⟨_, narrow_binop_mul_7⟩,
    ⟨_, narrow_binop_and_7⟩,
    ⟨_, narrow_binop_or_7⟩,
    ⟨_, narrow_binop_xor_7⟩,
    ⟨_, narrow_binop_add_8⟩,
    ⟨_, narrow_binop_sub_8⟩,
    ⟨_, narrow_binop_mul_8⟩,
    ⟨_, narrow_binop_and_8⟩,
    ⟨_, narrow_binop_or_8⟩,
    ⟨_, narrow_binop_xor_8⟩,
    ⟨_, narrow_binop_add_9⟩,
    ⟨_, narrow_binop_sub_9⟩,
    ⟨_, narrow_binop_mul_9⟩,
    ⟨_, narrow_binop_and_9⟩,
    ⟨_, narrow_binop_or_9⟩,
    ⟨_, narrow_binop_xor_9⟩,
    ⟨_, narrow_binop_add_10⟩,
    ⟨_, narrow_binop_sub_10⟩,
    ⟨_, narrow_binop_mul_10⟩,
    ⟨_, narrow_binop_and_10⟩,
    ⟨_, narrow_binop_or_10⟩,
    ⟨_, narrow_binop_xor_10⟩,
    ⟨_, narrow_binop_add_11⟩,
    ⟨_, narrow_binop_sub_11⟩,
    ⟨_, narrow_binop_mul_11⟩,
    ⟨_, narrow_binop_and_11⟩,
    ⟨_, narrow_binop_or_11⟩,
    ⟨_, narrow_binop_xor_11⟩,
    ⟨_, narrow_binop_add_12⟩,
    ⟨_, narrow_binop_sub_12⟩,
    ⟨_, narrow_binop_mul_12⟩,
    ⟨_, narrow_binop_and_12⟩,
    ⟨_, narrow_binop_or_12⟩,
    ⟨_, narrow_binop_xor_12⟩,
    ⟨_, narrow_binop_add_13⟩,
    ⟨_, narrow_binop_sub_13⟩,
    ⟨_, narrow_binop_mul_13⟩,
    ⟨_, narrow_binop_and_13⟩,
    ⟨_, narrow_binop_or_13⟩,
    ⟨_, narrow_binop_xor_13⟩,
    ⟨_, narrow_binop_add_14⟩,
    ⟨_, narrow_binop_sub_14⟩,
    ⟨_, narrow_binop_mul_14⟩,
    ⟨_, narrow_binop_and_14⟩,
    ⟨_, narrow_binop_or_14⟩,
    ⟨_, narrow_binop_xor_14⟩,
    ⟨_, narrow_binop_add_15⟩,
    ⟨_, narrow_binop_sub_15⟩,
    ⟨_, narrow_binop_mul_15⟩,
    ⟨_, narrow_binop_and_15⟩,
    ⟨_, narrow_binop_or_15⟩,
    ⟨_, narrow_binop_xor_15⟩,
    ⟨_, narrow_binop_add_16⟩,
    ⟨_, narrow_binop_sub_16⟩,
    ⟨_, narrow_binop_mul_16⟩,
    ⟨_, narrow_binop_and_16⟩,
    ⟨_, narrow_binop_or_16⟩,
    ⟨_, narrow_binop_xor_16⟩,
    ⟨_, narrow_binop_add_17⟩,
    ⟨_, narrow_binop_sub_17⟩,
    ⟨_, narrow_binop_mul_17⟩,
    ⟨_, narrow_binop_and_17⟩,
    ⟨_, narrow_binop_or_17⟩,
    ⟨_, narrow_binop_xor_17⟩,
    ⟨_, narrow_binop_add_18⟩,
    ⟨_, narrow_binop_sub_18⟩,
    ⟨_, narrow_binop_mul_18⟩,
    ⟨_, narrow_binop_and_18⟩,
    ⟨_, narrow_binop_or_18⟩,
    ⟨_, narrow_binop_xor_18⟩,
    ⟨_, narrow_binop_add_19⟩,
    ⟨_, narrow_binop_sub_19⟩,
    ⟨_, narrow_binop_mul_19⟩,
    ⟨_, narrow_binop_and_19⟩,
    ⟨_, narrow_binop_or_19⟩,
    ⟨_, narrow_binop_xor_19⟩,
    ⟨_, narrow_binop_add_20⟩,
    ⟨_, narrow_binop_sub_20⟩,
    ⟨_, narrow_binop_mul_20⟩,
    ⟨_, narrow_binop_and_20⟩,
    ⟨_, narrow_binop_or_20⟩,
    ⟨_, narrow_binop_xor_20⟩,
    ⟨_, narrow_binop_add_21⟩,
    ⟨_, narrow_binop_sub_21⟩,
    ⟨_, narrow_binop_mul_21⟩,
    ⟨_, narrow_binop_and_21⟩,
    ⟨_, narrow_binop_or_21⟩,
    ⟨_, narrow_binop_xor_21⟩,
    ⟨_, narrow_binop_add_22⟩,
    ⟨_, narrow_binop_sub_22⟩,
    ⟨_, narrow_binop_mul_22⟩,
    ⟨_, narrow_binop_and_22⟩,
    ⟨_, narrow_binop_or_22⟩,
    ⟨_, narrow_binop_xor_22⟩,
    ⟨_, narrow_binop_add_23⟩,
    ⟨_, narrow_binop_sub_23⟩,
    ⟨_, narrow_binop_mul_23⟩,
    ⟨_, narrow_binop_and_23⟩,
    ⟨_, narrow_binop_or_23⟩,
    ⟨_, narrow_binop_xor_23⟩,
    ⟨_, narrow_binop_add_24⟩,
    ⟨_, narrow_binop_sub_24⟩,
    ⟨_, narrow_binop_mul_24⟩,
    ⟨_, narrow_binop_and_24⟩,
    ⟨_, narrow_binop_or_24⟩,
    ⟨_, narrow_binop_xor_24⟩,
    ⟨_, narrow_binop_add_25⟩,
    ⟨_, narrow_binop_sub_25⟩,
    ⟨_, narrow_binop_mul_25⟩,
    ⟨_, narrow_binop_and_25⟩,
    ⟨_, narrow_binop_or_25⟩,
    ⟨_, narrow_binop_xor_25⟩,
    ⟨_, narrow_binop_add_26⟩,
    ⟨_, narrow_binop_sub_26⟩,
    ⟨_, narrow_binop_mul_26⟩,
    ⟨_, narrow_binop_and_26⟩,
    ⟨_, narrow_binop_or_26⟩,
    ⟨_, narrow_binop_xor_26⟩,
    ⟨_, narrow_binop_add_27⟩,
    ⟨_, narrow_binop_sub_27⟩,
    ⟨_, narrow_binop_mul_27⟩,
    ⟨_, narrow_binop_and_27⟩,
    ⟨_, narrow_binop_or_27⟩,
    ⟨_, narrow_binop_xor_27⟩,
    ⟨_, narrow_binop_add_28⟩,
    ⟨_, narrow_binop_sub_28⟩,
    ⟨_, narrow_binop_mul_28⟩,
    ⟨_, narrow_binop_and_28⟩,
    ⟨_, narrow_binop_or_28⟩,
    ⟨_, narrow_binop_xor_28⟩,
    ⟨_, narrow_binop_add_29⟩,
    ⟨_, narrow_binop_sub_29⟩,
    ⟨_, narrow_binop_mul_29⟩,
    ⟨_, narrow_binop_and_29⟩,
    ⟨_, narrow_binop_or_29⟩,
    ⟨_, narrow_binop_xor_29⟩,
    ⟨_, narrow_binop_add_30⟩,
    ⟨_, narrow_binop_sub_30⟩,
    ⟨_, narrow_binop_mul_30⟩,
    ⟨_, narrow_binop_and_30⟩,
    ⟨_, narrow_binop_or_30⟩,
    ⟨_, narrow_binop_xor_30⟩,
    ⟨_, narrow_binop_add_31⟩,
    ⟨_, narrow_binop_sub_31⟩,
    ⟨_, narrow_binop_mul_31⟩,
    ⟨_, narrow_binop_and_31⟩,
    ⟨_, narrow_binop_or_31⟩,
    ⟨_, narrow_binop_xor_31⟩,
    ⟨_, narrow_binop_add_32⟩,
    ⟨_, narrow_binop_sub_32⟩,
    ⟨_, narrow_binop_mul_32⟩,
    ⟨_, narrow_binop_and_32⟩,
    ⟨_, narrow_binop_or_32⟩,
    ⟨_, narrow_binop_xor_32⟩,
    ⟨_, narrow_binop_add_33⟩,
    ⟨_, narrow_binop_sub_33⟩,
    ⟨_, narrow_binop_mul_33⟩,
    ⟨_, narrow_binop_and_33⟩,
    ⟨_, narrow_binop_or_33⟩,
    ⟨_, narrow_binop_xor_33⟩,
    ⟨_, narrow_binop_add_34⟩,
    ⟨_, narrow_binop_sub_34⟩,
    ⟨_, narrow_binop_mul_34⟩,
    ⟨_, narrow_binop_and_34⟩,
    ⟨_, narrow_binop_or_34⟩,
    ⟨_, narrow_binop_xor_34⟩,
    ⟨_, narrow_binop_add_35⟩,
    ⟨_, narrow_binop_sub_35⟩,
    ⟨_, narrow_binop_mul_35⟩,
    ⟨_, narrow_binop_and_35⟩,
    ⟨_, narrow_binop_or_35⟩,
    ⟨_, narrow_binop_xor_35⟩,
    ⟨_, narrow_binop_add_36⟩,
    ⟨_, narrow_binop_sub_36⟩,
    ⟨_, narrow_binop_mul_36⟩,
    ⟨_, narrow_binop_and_36⟩,
    ⟨_, narrow_binop_or_36⟩,
    ⟨_, narrow_binop_xor_36⟩,
    ⟨_, narrow_binop_add_37⟩,
    ⟨_, narrow_binop_sub_37⟩,
    ⟨_, narrow_binop_mul_37⟩,
    ⟨_, narrow_binop_and_37⟩,
    ⟨_, narrow_binop_or_37⟩,
    ⟨_, narrow_binop_xor_37⟩,
    ⟨_, narrow_binop_add_38⟩,
    ⟨_, narrow_binop_sub_38⟩,
    ⟨_, narrow_binop_mul_38⟩,
    ⟨_, narrow_binop_and_38⟩,
    ⟨_, narrow_binop_or_38⟩,
    ⟨_, narrow_binop_xor_38⟩,
    ⟨_, narrow_binop_add_39⟩,
    ⟨_, narrow_binop_sub_39⟩,
    ⟨_, narrow_binop_mul_39⟩,
    ⟨_, narrow_binop_and_39⟩,
    ⟨_, narrow_binop_or_39⟩,
    ⟨_, narrow_binop_xor_39⟩,
    ⟨_, narrow_binop_add_40⟩,
    ⟨_, narrow_binop_sub_40⟩,
    ⟨_, narrow_binop_mul_40⟩,
    ⟨_, narrow_binop_and_40⟩,
    ⟨_, narrow_binop_or_40⟩,
    ⟨_, narrow_binop_xor_40⟩,
    ⟨_, narrow_binop_add_41⟩,
    ⟨_, narrow_binop_sub_41⟩,
    ⟨_, narrow_binop_mul_41⟩,
    ⟨_, narrow_binop_and_41⟩,
    ⟨_, narrow_binop_or_41⟩,
    ⟨_, narrow_binop_xor_41⟩,
    ⟨_, narrow_binop_add_42⟩,
    ⟨_, narrow_binop_sub_42⟩,
    ⟨_, narrow_binop_mul_42⟩,
    ⟨_, narrow_binop_and_42⟩,
    ⟨_, narrow_binop_or_42⟩,
    ⟨_, narrow_binop_xor_42⟩,
    ⟨_, narrow_binop_add_43⟩,
    ⟨_, narrow_binop_sub_43⟩,
    ⟨_, narrow_binop_mul_43⟩,
    ⟨_, narrow_binop_and_43⟩,
    ⟨_, narrow_binop_or_43⟩,
    ⟨_, narrow_binop_xor_43⟩,
    ⟨_, narrow_binop_add_44⟩,
    ⟨_, narrow_binop_sub_44⟩,
    ⟨_, narrow_binop_mul_44⟩,
    ⟨_, narrow_binop_and_44⟩,
    ⟨_, narrow_binop_or_44⟩,
    ⟨_, narrow_binop_xor_44⟩,
    ⟨_, narrow_binop_add_45⟩,
    ⟨_, narrow_binop_sub_45⟩,
    ⟨_, narrow_binop_mul_45⟩,
    ⟨_, narrow_binop_and_45⟩,
    ⟨_, narrow_binop_or_45⟩,
    ⟨_, narrow_binop_xor_45⟩,
    ⟨_, narrow_binop_add_46⟩,
    ⟨_, narrow_binop_sub_46⟩,
    ⟨_, narrow_binop_mul_46⟩,
    ⟨_, narrow_binop_and_46⟩,
    ⟨_, narrow_binop_or_46⟩,
    ⟨_, narrow_binop_xor_46⟩,
    ⟨_, narrow_binop_add_47⟩,
    ⟨_, narrow_binop_sub_47⟩,
    ⟨_, narrow_binop_mul_47⟩,
    ⟨_, narrow_binop_and_47⟩,
    ⟨_, narrow_binop_or_47⟩,
    ⟨_, narrow_binop_xor_47⟩,
    ⟨_, narrow_binop_add_48⟩,
    ⟨_, narrow_binop_sub_48⟩,
    ⟨_, narrow_binop_mul_48⟩,
    ⟨_, narrow_binop_and_48⟩,
    ⟨_, narrow_binop_or_48⟩,
    ⟨_, narrow_binop_xor_48⟩,
    ⟨_, narrow_binop_add_49⟩,
    ⟨_, narrow_binop_sub_49⟩,
    ⟨_, narrow_binop_mul_49⟩,
    ⟨_, narrow_binop_and_49⟩,
    ⟨_, narrow_binop_or_49⟩,
    ⟨_, narrow_binop_xor_49⟩,
    ⟨_, narrow_binop_add_50⟩,
    ⟨_, narrow_binop_sub_50⟩,
    ⟨_, narrow_binop_mul_50⟩,
    ⟨_, narrow_binop_and_50⟩,
    ⟨_, narrow_binop_or_50⟩,
    ⟨_, narrow_binop_xor_50⟩
  ]

/-! ### integer_reassoc_combines_constants -/

/-
Test the rewrite:
 fold (A+C1)-C2 -> A+(C1-C2)
 fold C2-(A+C1) -> (C2-C1)-A
 fold (A-C1)-C2 -> A-(C1+C2)
 fold (C1-A)-C2 -> (C1-C2)-A
 fold ((A-C1)+C2) -> (A+(C2-C1))
-/

def irc_constants_APlusC1MinusC2_neg2_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_neg2_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_neg2_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_neg2_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_neg2_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_neg2_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_neg2_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_neg2_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_neg2_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_neg2_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_neg2_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_neg2_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_neg2_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_neg2_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_neg2_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_neg2_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_neg2_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_neg2_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_neg2_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_neg2_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_neg2_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_neg2_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_neg2_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_neg2_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_neg2_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_neg1_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_neg1_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_neg1_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_neg1_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_neg1_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_neg1_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_neg1_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_neg1_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_neg1_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_neg1_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_neg1_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_neg1_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_neg1_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_neg1_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_neg1_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_neg1_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_neg1_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_neg1_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_neg1_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_neg1_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_neg1_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_neg1_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_neg1_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_neg1_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_neg1_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (-1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_0_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_0_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_0_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_0_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_0_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_0_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_0_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_0_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_0_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_0_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_0_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_0_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_0_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_0_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_0_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_0_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_0_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_0_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_0_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_0_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_0_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_0_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_0_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_0_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_0_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (0) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_1_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_1_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_1_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_1_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_1_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_1_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_1_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_1_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_1_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_1_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_1_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_1_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_1_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_1_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_1_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_1_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_1_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_1_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_1_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_1_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_1_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_1_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_1_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_1_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_1_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (1) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_2_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_2_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_2_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_2_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_2_neg2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_2_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_2_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_2_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_2_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_2_neg1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (-1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_2_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_2_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_2_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_2_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_2_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (0) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_2_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_2_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_2_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_2_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_2_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_APlusC1MinusC2_2_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C2MinusAPlusC1_2_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.add %a, %c1 : i64
      %1 = llvm.sub %c2, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1MinusC2_2_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.add %c1, %c2 : i64
      %1 = llvm.sub %a, %0 : i64
      llvm.return %1 : i64
  }]

def irc_constants_C1Minus2MinusC2_2_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c1, %a : i64
      %1 = llvm.sub %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c1, %c2 : i64
      %1 = llvm.sub %0, %a : i64
      llvm.return %1 : i64
  }]

def irc_constants_AMinusC1PlusC2_2_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %a, %c1 : i64
      %1 = llvm.add %0, %c2 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%a: i64):
      %c1 = llvm.mlir.constant (2) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.sub %c2, %c1 : i64
      %1 = llvm.add %a, %0 : i64
      llvm.return %1 : i64
  }]
def irc_constants : List (Σ Γ, LLVMPeepholeRewriteRefine 64 Γ) :=
  [
    ⟨_, irc_constants_APlusC1MinusC2_neg2_neg2⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_neg2_neg2⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_neg2_neg2⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_neg2_neg2⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_neg2_neg2⟩,
    ⟨_, irc_constants_APlusC1MinusC2_neg2_neg1⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_neg2_neg1⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_neg2_neg1⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_neg2_neg1⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_neg2_neg1⟩,
    ⟨_, irc_constants_APlusC1MinusC2_neg2_0⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_neg2_0⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_neg2_0⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_neg2_0⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_neg2_0⟩,
    ⟨_, irc_constants_APlusC1MinusC2_neg2_1⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_neg2_1⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_neg2_1⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_neg2_1⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_neg2_1⟩,
    ⟨_, irc_constants_APlusC1MinusC2_neg2_2⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_neg2_2⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_neg2_2⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_neg2_2⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_neg2_2⟩,
    ⟨_, irc_constants_APlusC1MinusC2_neg1_neg2⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_neg1_neg2⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_neg1_neg2⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_neg1_neg2⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_neg1_neg2⟩,
    ⟨_, irc_constants_APlusC1MinusC2_neg1_neg1⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_neg1_neg1⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_neg1_neg1⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_neg1_neg1⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_neg1_neg1⟩,
    ⟨_, irc_constants_APlusC1MinusC2_neg1_0⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_neg1_0⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_neg1_0⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_neg1_0⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_neg1_0⟩,
    ⟨_, irc_constants_APlusC1MinusC2_neg1_1⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_neg1_1⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_neg1_1⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_neg1_1⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_neg1_1⟩,
    ⟨_, irc_constants_APlusC1MinusC2_neg1_2⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_neg1_2⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_neg1_2⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_neg1_2⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_neg1_2⟩,
    ⟨_, irc_constants_APlusC1MinusC2_0_neg2⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_0_neg2⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_0_neg2⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_0_neg2⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_0_neg2⟩,
    ⟨_, irc_constants_APlusC1MinusC2_0_neg1⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_0_neg1⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_0_neg1⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_0_neg1⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_0_neg1⟩,
    ⟨_, irc_constants_APlusC1MinusC2_0_0⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_0_0⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_0_0⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_0_0⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_0_0⟩,
    ⟨_, irc_constants_APlusC1MinusC2_0_1⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_0_1⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_0_1⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_0_1⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_0_1⟩,
    ⟨_, irc_constants_APlusC1MinusC2_0_2⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_0_2⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_0_2⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_0_2⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_0_2⟩,
    ⟨_, irc_constants_APlusC1MinusC2_1_neg2⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_1_neg2⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_1_neg2⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_1_neg2⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_1_neg2⟩,
    ⟨_, irc_constants_APlusC1MinusC2_1_neg1⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_1_neg1⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_1_neg1⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_1_neg1⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_1_neg1⟩,
    ⟨_, irc_constants_APlusC1MinusC2_1_0⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_1_0⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_1_0⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_1_0⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_1_0⟩,
    ⟨_, irc_constants_APlusC1MinusC2_1_1⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_1_1⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_1_1⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_1_1⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_1_1⟩,
    ⟨_, irc_constants_APlusC1MinusC2_1_2⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_1_2⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_1_2⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_1_2⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_1_2⟩,
    ⟨_, irc_constants_APlusC1MinusC2_2_neg2⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_2_neg2⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_2_neg2⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_2_neg2⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_2_neg2⟩,
    ⟨_, irc_constants_APlusC1MinusC2_2_neg1⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_2_neg1⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_2_neg1⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_2_neg1⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_2_neg1⟩,
    ⟨_, irc_constants_APlusC1MinusC2_2_0⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_2_0⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_2_0⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_2_0⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_2_0⟩,
    ⟨_, irc_constants_APlusC1MinusC2_2_1⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_2_1⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_2_1⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_2_1⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_2_1⟩,
    ⟨_, irc_constants_APlusC1MinusC2_2_2⟩,
    ⟨_, irc_constants_C2MinusAPlusC1_2_2⟩,
    ⟨_, irc_constants_AMinusC1MinusC2_2_2⟩,
    ⟨_, irc_constants_C1Minus2MinusC2_2_2⟩,
    ⟨_, irc_constants_AMinusC1PlusC2_2_2⟩
  ]

/-! ### udiv_by_pow2 -/

/--
Test the rewrite:
    udiv(x, 2^n) -> x >> (n)
-/
def udiv_pow2_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1) : i64
      %0 = llvm.udiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (0) : i64
      %0 = llvm.lshr %x, %c : i64
      llvm.return %0 : i64
  }]

def udiv_pow2_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2) : i64
      %0 = llvm.udiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (1) : i64
      %0 = llvm.lshr %x, %c : i64
      llvm.return %0 : i64
  }]

def udiv_pow2_4 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4) : i64
      %0 = llvm.udiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2) : i64
      %0 = llvm.lshr %x, %c : i64
      llvm.return %0 : i64
  }]

def udiv_pow2_8 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8) : i64
      %0 = llvm.udiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (3) : i64
      %0 = llvm.lshr %x, %c : i64
      llvm.return %0 : i64
  }]

def udiv_pow2_16 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (16) : i64
      %0 = llvm.udiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4) : i64
      %0 = llvm.lshr %x, %c : i64
      llvm.return %0 : i64
  }]

def udiv_pow2_32 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (32) : i64
      %0 = llvm.udiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (5) : i64
      %0 = llvm.lshr %x, %c : i64
      llvm.return %0 : i64
  }]

def udiv_pow2_64 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (64) : i64
      %0 = llvm.udiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (6) : i64
      %0 = llvm.lshr %x, %c : i64
      llvm.return %0 : i64
  }]

def udiv_pow2_128 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (128) : i64
      %0 = llvm.udiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (7) : i64
      %0 = llvm.lshr %x, %c : i64
      llvm.return %0 : i64
  }]

def udiv_pow2_256 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (256) : i64
      %0 = llvm.udiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8) : i64
      %0 = llvm.lshr %x, %c : i64
      llvm.return %0 : i64
  }]

def udiv_pow2_512 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (512) : i64
      %0 = llvm.udiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (9) : i64
      %0 = llvm.lshr %x, %c : i64
      llvm.return %0 : i64
  }]

def udiv_pow2 : List (Σ Γ, LLVMPeepholeRewriteRefine 64 Γ) :=
  [
    ⟨_, udiv_pow2_1⟩,
    ⟨_, udiv_pow2_2⟩,
    ⟨_, udiv_pow2_4⟩,
    ⟨_, udiv_pow2_8⟩,
    ⟨_, udiv_pow2_16⟩,
    ⟨_, udiv_pow2_32⟩,
    ⟨_, udiv_pow2_64⟩,
    ⟨_, udiv_pow2_128⟩,
    ⟨_, udiv_pow2_256⟩,
    ⟨_, udiv_pow2_512⟩
  ]

/-! ### sdiv_by_pow2 -/

/--
Test the rewrite:
    sdiv(x, 2^n) -> x >> (n)
-/
def sdiv_pow2_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (2) : i64
      %0 = llvm.sdiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c0 = llvm.mlir.constant (63) : i64
      %c1 = llvm.mlir.constant (63) : i64
      %c2 = llvm.mlir.constant (1) : i64
      %0 = llvm.ashr %x, %c0 : i64
      %1 = llvm.lshr %0, %c1 : i64
      %2 = llvm.add %x, %1 : i64
      %3 = llvm.ashr %2, %c2 : i64
      llvm.return %3 : i64
  }]

def sdiv_pow2_4 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (4) : i64
      %0 = llvm.sdiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c0 = llvm.mlir.constant (63) : i64
      %c1 = llvm.mlir.constant (62) : i64
      %c2 = llvm.mlir.constant (2) : i64
      %0 = llvm.ashr %x, %c0 : i64
      %1 = llvm.lshr %0, %c1 : i64
      %2 = llvm.add %x, %1 : i64
      %3 = llvm.ashr %2, %c2 : i64
      llvm.return %3 : i64
  }]

def sdiv_pow2_8 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (8) : i64
      %0 = llvm.sdiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c0 = llvm.mlir.constant (63) : i64
      %c1 = llvm.mlir.constant (61) : i64
      %c2 = llvm.mlir.constant (3) : i64
      %0 = llvm.ashr %x, %c0 : i64
      %1 = llvm.lshr %0, %c1 : i64
      %2 = llvm.add %x, %1 : i64
      %3 = llvm.ashr %2, %c2 : i64
      llvm.return %3 : i64
  }]

def sdiv_pow2_16 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (16) : i64
      %0 = llvm.sdiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c0 = llvm.mlir.constant (63) : i64
      %c1 = llvm.mlir.constant (60) : i64
      %c2 = llvm.mlir.constant (4) : i64
      %0 = llvm.ashr %x, %c0 : i64
      %1 = llvm.lshr %0, %c1 : i64
      %2 = llvm.add %x, %1 : i64
      %3 = llvm.ashr %2, %c2 : i64
      llvm.return %3 : i64
  }]

def sdiv_pow2_32 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (32) : i64
      %0 = llvm.sdiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c0 = llvm.mlir.constant (63) : i64
      %c1 = llvm.mlir.constant (59) : i64
      %c2 = llvm.mlir.constant (5) : i64
      %0 = llvm.ashr %x, %c0 : i64
      %1 = llvm.lshr %0, %c1 : i64
      %2 = llvm.add %x, %1 : i64
      %3 = llvm.ashr %2, %c2 : i64
      llvm.return %3 : i64
  }]

def sdiv_pow2_64 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (64) : i64
      %0 = llvm.sdiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c0 = llvm.mlir.constant (63) : i64
      %c1 = llvm.mlir.constant (58) : i64
      %c2 = llvm.mlir.constant (6) : i64
      %0 = llvm.ashr %x, %c0 : i64
      %1 = llvm.lshr %0, %c1 : i64
      %2 = llvm.add %x, %1 : i64
      %3 = llvm.ashr %2, %c2 : i64
      llvm.return %3 : i64
  }]

def sdiv_pow2_128 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (128) : i64
      %0 = llvm.sdiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c0 = llvm.mlir.constant (63) : i64
      %c1 = llvm.mlir.constant (57) : i64
      %c2 = llvm.mlir.constant (7) : i64
      %0 = llvm.ashr %x, %c0 : i64
      %1 = llvm.lshr %0, %c1 : i64
      %2 = llvm.add %x, %1 : i64
      %3 = llvm.ashr %2, %c2 : i64
      llvm.return %3 : i64
  }]

def sdiv_pow2_256 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (256) : i64
      %0 = llvm.sdiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c0 = llvm.mlir.constant (63) : i64
      %c1 = llvm.mlir.constant (56) : i64
      %c2 = llvm.mlir.constant (8) : i64
      %0 = llvm.ashr %x, %c0 : i64
      %1 = llvm.lshr %0, %c1 : i64
      %2 = llvm.add %x, %1 : i64
      %3 = llvm.ashr %2, %c2 : i64
      llvm.return %3 : i64
  }]

def sdiv_pow2_512 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%x: i64):
      %c = llvm.mlir.constant (512) : i64
      %0 = llvm.sdiv %x, %c : i64
      llvm.return %0 : i64
  }]
  rhs := [LV| {
    ^entry (%x: i64):
      %c0 = llvm.mlir.constant (63) : i64
      %c1 = llvm.mlir.constant (55) : i64
      %c2 = llvm.mlir.constant (9) : i64
      %0 = llvm.ashr %x, %c0 : i64
      %1 = llvm.lshr %0, %c1 : i64
      %2 = llvm.add %x, %1 : i64
      %3 = llvm.ashr %2, %c2 : i64
      llvm.return %3 : i64
  }]

def sdiv_pow2 : List (Σ Γ, LLVMPeepholeRewriteRefine 64 Γ) :=
  [
    ⟨_, sdiv_pow2_2⟩,
    ⟨_, sdiv_pow2_4⟩,
    ⟨_, sdiv_pow2_8⟩,
    ⟨_, sdiv_pow2_16⟩,
    ⟨_, sdiv_pow2_32⟩,
    ⟨_, sdiv_pow2_64⟩,
    ⟨_, sdiv_pow2_128⟩,
    ⟨_, sdiv_pow2_256⟩,
    ⟨_, sdiv_pow2_512⟩
  ]

/-- We group all the rewrites that depend constant folding to optimize the program. Without constant folding, these rewrites would either increase the instruction count, or do not result in any optimization. -/
def GlobalISelPostLegalizerCombinerConstantFolding :
  List (Σ Γ, Σ ty, PeepholeRewrite LLVMPlusRiscV Γ ty) :=
    (List.map (fun ⟨_,y⟩ => mkRewrite (LLVMToRiscvPeepholeRewriteRefine.toPeepholeUNSOUND y))
    irc_constants) ++
    (List.map (fun ⟨_,y⟩ => mkRewrite (LLVMToRiscvPeepholeRewriteRefine.toPeepholeUNSOUND y))
    sdiv_pow2)
