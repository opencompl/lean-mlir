-- AUTOGENERATED Lean file: Constant lowering for RISC-V li instructions

import SSA.Projects.LLVMRiscV.PeepholeRefine
import SSA.Projects.LLVMRiscV.simpproc
import SSA.Projects.RISCV64.Tactic.SimpRiscV

 open LLVMRiscV

    def li_riscv_n50 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -50 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n50 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-50) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin50 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n50, rhs:= li_riscv_n50,
   correct := by
    unfold const_llvm_n50 li_riscv_n50
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n49 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -49 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n49 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-49) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin49 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n49, rhs:= li_riscv_n49,
   correct := by
    unfold const_llvm_n49 li_riscv_n49
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n48 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -48 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n48 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-48) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin48 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n48, rhs:= li_riscv_n48,
   correct := by
    unfold const_llvm_n48 li_riscv_n48
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n47 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -47 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n47 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-47) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin47 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n47, rhs:= li_riscv_n47,
   correct := by
    unfold const_llvm_n47 li_riscv_n47
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n46 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -46 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n46 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-46) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin46 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n46, rhs:= li_riscv_n46,
   correct := by
    unfold const_llvm_n46 li_riscv_n46
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n45 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -45 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n45 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-45) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin45 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n45, rhs:= li_riscv_n45,
   correct := by
    unfold const_llvm_n45 li_riscv_n45
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n44 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -44 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n44 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-44) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin44 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n44, rhs:= li_riscv_n44,
   correct := by
    unfold const_llvm_n44 li_riscv_n44
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n43 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -43 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n43 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-43) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin43 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n43, rhs:= li_riscv_n43,
   correct := by
    unfold const_llvm_n43 li_riscv_n43
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n42 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -42 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n42 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-42) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin42 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n42, rhs:= li_riscv_n42,
   correct := by
    unfold const_llvm_n42 li_riscv_n42
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n41 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -41 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n41 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-41) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin41 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n41, rhs:= li_riscv_n41,
   correct := by
    unfold const_llvm_n41 li_riscv_n41
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n40 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -40 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n40 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-40) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin40 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n40, rhs:= li_riscv_n40,
   correct := by
    unfold const_llvm_n40 li_riscv_n40
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n39 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -39 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n39 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-39) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin39 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n39, rhs:= li_riscv_n39,
   correct := by
    unfold const_llvm_n39 li_riscv_n39
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n38 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -38 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n38 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-38) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin38 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n38, rhs:= li_riscv_n38,
   correct := by
    unfold const_llvm_n38 li_riscv_n38
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n37 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -37 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n37 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-37) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin37 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n37, rhs:= li_riscv_n37,
   correct := by
    unfold const_llvm_n37 li_riscv_n37
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n36 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -36 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n36 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-36) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin36 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n36, rhs:= li_riscv_n36,
   correct := by
    unfold const_llvm_n36 li_riscv_n36
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n35 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -35 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n35 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-35) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin35 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n35, rhs:= li_riscv_n35,
   correct := by
    unfold const_llvm_n35 li_riscv_n35
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n34 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -34 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n34 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-34) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin34 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n34, rhs:= li_riscv_n34,
   correct := by
    unfold const_llvm_n34 li_riscv_n34
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n33 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -33 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n33 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-33) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin33 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n33, rhs:= li_riscv_n33,
   correct := by
    unfold const_llvm_n33 li_riscv_n33
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n32 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -32 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n32 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-32) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin32 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n32, rhs:= li_riscv_n32,
   correct := by
    unfold const_llvm_n32 li_riscv_n32
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n31 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -31 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n31 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-31) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin31 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n31, rhs:= li_riscv_n31,
   correct := by
    unfold const_llvm_n31 li_riscv_n31
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n30 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -30 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n30 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-30) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin30 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n30, rhs:= li_riscv_n30,
   correct := by
    unfold const_llvm_n30 li_riscv_n30
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n29 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -29 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n29 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-29) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin29 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n29, rhs:= li_riscv_n29,
   correct := by
    unfold const_llvm_n29 li_riscv_n29
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n28 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -28 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n28 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-28) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin28 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n28, rhs:= li_riscv_n28,
   correct := by
    unfold const_llvm_n28 li_riscv_n28
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n27 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -27 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n27 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-27) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin27 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n27, rhs:= li_riscv_n27,
   correct := by
    unfold const_llvm_n27 li_riscv_n27
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n26 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -26 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n26 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-26) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin26 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n26, rhs:= li_riscv_n26,
   correct := by
    unfold const_llvm_n26 li_riscv_n26
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n25 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -25 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n25 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-25) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin25 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n25, rhs:= li_riscv_n25,
   correct := by
    unfold const_llvm_n25 li_riscv_n25
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n24 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -24 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n24 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-24) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin24 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n24, rhs:= li_riscv_n24,
   correct := by
    unfold const_llvm_n24 li_riscv_n24
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n23 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -23 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n23 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-23) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin23 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n23, rhs:= li_riscv_n23,
   correct := by
    unfold const_llvm_n23 li_riscv_n23
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n22 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -22 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n22 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-22) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin22 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n22, rhs:= li_riscv_n22,
   correct := by
    unfold const_llvm_n22 li_riscv_n22
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n21 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -21 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n21 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-21) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin21 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n21, rhs:= li_riscv_n21,
   correct := by
    unfold const_llvm_n21 li_riscv_n21
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n20 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -20 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n20 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-20) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin20 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n20, rhs:= li_riscv_n20,
   correct := by
    unfold const_llvm_n20 li_riscv_n20
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n19 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -19 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n19 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-19) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin19 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n19, rhs:= li_riscv_n19,
   correct := by
    unfold const_llvm_n19 li_riscv_n19
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n18 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -18 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n18 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-18) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin18 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n18, rhs:= li_riscv_n18,
   correct := by
    unfold const_llvm_n18 li_riscv_n18
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n17 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -17 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n17 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-17) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin17 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n17, rhs:= li_riscv_n17,
   correct := by
    unfold const_llvm_n17 li_riscv_n17
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n16 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -16 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n16 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-16) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin16 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n16, rhs:= li_riscv_n16,
   correct := by
    unfold const_llvm_n16 li_riscv_n16
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n15 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -15 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n15 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-15) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin15 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n15, rhs:= li_riscv_n15,
   correct := by
    unfold const_llvm_n15 li_riscv_n15
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n14 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -14 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n14 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-14) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin14 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n14, rhs:= li_riscv_n14,
   correct := by
    unfold const_llvm_n14 li_riscv_n14
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n13 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -13 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n13 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-13) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin13 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n13, rhs:= li_riscv_n13,
   correct := by
    unfold const_llvm_n13 li_riscv_n13
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n12 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -12 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n12 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-12) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin12 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n12, rhs:= li_riscv_n12,
   correct := by
    unfold const_llvm_n12 li_riscv_n12
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n11 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -11 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n11 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-11) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin11 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n11, rhs:= li_riscv_n11,
   correct := by
    unfold const_llvm_n11 li_riscv_n11
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n10 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -10 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n10 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-10) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin10 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n10, rhs:= li_riscv_n10,
   correct := by
    unfold const_llvm_n10 li_riscv_n10
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n9 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -9 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n9 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-9) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin9 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n9, rhs:= li_riscv_n9,
   correct := by
    unfold const_llvm_n9 li_riscv_n9
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n8 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -8 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n8 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-8) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin8 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n8, rhs:= li_riscv_n8,
   correct := by
    unfold const_llvm_n8 li_riscv_n8
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n7 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -7 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n7 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-7) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin7 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n7, rhs:= li_riscv_n7,
   correct := by
    unfold const_llvm_n7 li_riscv_n7
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n6 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -6 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n6 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-6) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin6 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n6, rhs:= li_riscv_n6,
   correct := by
    unfold const_llvm_n6 li_riscv_n6
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n5 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -5 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n5 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-5) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin5 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n5, rhs:= li_riscv_n5,
   correct := by
    unfold const_llvm_n5 li_riscv_n5
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n4 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -4 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n4 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-4) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin4 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n4, rhs:= li_riscv_n4,
   correct := by
    unfold const_llvm_n4 li_riscv_n4
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n3 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -3 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n3 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-3) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin3 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n3, rhs:= li_riscv_n3,
   correct := by
    unfold const_llvm_n3 li_riscv_n3
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n2 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -2 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n2 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-2) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin2 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n2, rhs:= li_riscv_n2,
   correct := by
    unfold const_llvm_n2 li_riscv_n2
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_n1 := [LV| {
    ^entry ():
      %0 = "li"() {imm = -1 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_n1 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (-1) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_lin1 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_n1, rhs:= li_riscv_n1,
   correct := by
    unfold const_llvm_n1 li_riscv_n1
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_0 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 0 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_0 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (0) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li0 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_0, rhs:= li_riscv_0,
   correct := by
    unfold const_llvm_0 li_riscv_0
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_1 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 1 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_1 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (1) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li1 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_1, rhs:= li_riscv_1,
   correct := by
    unfold const_llvm_1 li_riscv_1
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_2 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 2 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_2 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (2) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li2 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_2, rhs:= li_riscv_2,
   correct := by
    unfold const_llvm_2 li_riscv_2
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_3 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 3 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_3 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (3) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li3 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_3, rhs:= li_riscv_3,
   correct := by
    unfold const_llvm_3 li_riscv_3
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_4 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 4 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_4 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (4) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li4 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_4, rhs:= li_riscv_4,
   correct := by
    unfold const_llvm_4 li_riscv_4
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_5 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 5 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_5 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (5) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li5 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_5, rhs:= li_riscv_5,
   correct := by
    unfold const_llvm_5 li_riscv_5
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_6 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 6 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_6 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (6) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li6 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_6, rhs:= li_riscv_6,
   correct := by
    unfold const_llvm_6 li_riscv_6
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_7 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 7 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_7 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (7) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li7 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_7, rhs:= li_riscv_7,
   correct := by
    unfold const_llvm_7 li_riscv_7
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_8 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 8 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_8 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (8) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li8 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_8, rhs:= li_riscv_8,
   correct := by
    unfold const_llvm_8 li_riscv_8
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_9 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 9 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_9 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (9) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li9 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_9, rhs:= li_riscv_9,
   correct := by
    unfold const_llvm_9 li_riscv_9
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_10 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 10 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_10 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (10) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li10 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_10, rhs:= li_riscv_10,
   correct := by
    unfold const_llvm_10 li_riscv_10
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_11 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 11 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_11 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (11) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li11 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_11, rhs:= li_riscv_11,
   correct := by
    unfold const_llvm_11 li_riscv_11
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_12 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 12 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_12 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (12) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li12 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_12, rhs:= li_riscv_12,
   correct := by
    unfold const_llvm_12 li_riscv_12
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_13 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 13 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_13 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (13) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li13 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_13, rhs:= li_riscv_13,
   correct := by
    unfold const_llvm_13 li_riscv_13
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_14 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 14 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_14 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (14) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li14 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_14, rhs:= li_riscv_14,
   correct := by
    unfold const_llvm_14 li_riscv_14
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_15 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 15 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_15 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (15) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li15 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_15, rhs:= li_riscv_15,
   correct := by
    unfold const_llvm_15 li_riscv_15
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_16 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 16 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_16 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (16) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li16 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_16, rhs:= li_riscv_16,
   correct := by
    unfold const_llvm_16 li_riscv_16
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_17 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 17 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_17 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (17) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li17 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_17, rhs:= li_riscv_17,
   correct := by
    unfold const_llvm_17 li_riscv_17
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_18 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 18 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_18 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (18) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li18 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_18, rhs:= li_riscv_18,
   correct := by
    unfold const_llvm_18 li_riscv_18
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_19 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 19 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_19 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (19) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li19 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_19, rhs:= li_riscv_19,
   correct := by
    unfold const_llvm_19 li_riscv_19
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_20 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 20 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_20 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (20) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li20 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_20, rhs:= li_riscv_20,
   correct := by
    unfold const_llvm_20 li_riscv_20
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_21 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 21 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_21 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (21) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li21 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_21, rhs:= li_riscv_21,
   correct := by
    unfold const_llvm_21 li_riscv_21
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_22 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 22 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_22 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (22) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li22 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_22, rhs:= li_riscv_22,
   correct := by
    unfold const_llvm_22 li_riscv_22
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_23 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 23 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_23 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (23) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li23 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_23, rhs:= li_riscv_23,
   correct := by
    unfold const_llvm_23 li_riscv_23
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_24 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 24 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_24 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (24) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li24 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_24, rhs:= li_riscv_24,
   correct := by
    unfold const_llvm_24 li_riscv_24
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_25 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 25 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_25 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (25) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li25 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_25, rhs:= li_riscv_25,
   correct := by
    unfold const_llvm_25 li_riscv_25
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_26 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 26 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_26 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (26) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li26 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_26, rhs:= li_riscv_26,
   correct := by
    unfold const_llvm_26 li_riscv_26
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_27 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 27 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_27 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (27) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li27 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_27, rhs:= li_riscv_27,
   correct := by
    unfold const_llvm_27 li_riscv_27
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_28 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 28 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_28 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (28) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li28 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_28, rhs:= li_riscv_28,
   correct := by
    unfold const_llvm_28 li_riscv_28
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_29 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 29 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_29 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (29) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li29 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_29, rhs:= li_riscv_29,
   correct := by
    unfold const_llvm_29 li_riscv_29
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_30 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 30 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_30 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (30) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li30 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_30, rhs:= li_riscv_30,
   correct := by
    unfold const_llvm_30 li_riscv_30
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_31 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 31 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_31 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (31) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li31 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_31, rhs:= li_riscv_31,
   correct := by
    unfold const_llvm_31 li_riscv_31
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_32 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 32 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_32 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (32) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li32 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_32, rhs:= li_riscv_32,
   correct := by
    unfold const_llvm_32 li_riscv_32
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_33 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 33 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_33 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (33) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li33 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_33, rhs:= li_riscv_33,
   correct := by
    unfold const_llvm_33 li_riscv_33
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_34 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 34 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_34 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (34) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li34 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_34, rhs:= li_riscv_34,
   correct := by
    unfold const_llvm_34 li_riscv_34
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_35 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 35 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_35 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (35) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li35 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_35, rhs:= li_riscv_35,
   correct := by
    unfold const_llvm_35 li_riscv_35
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_36 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 36 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_36 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (36) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li36 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_36, rhs:= li_riscv_36,
   correct := by
    unfold const_llvm_36 li_riscv_36
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_37 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 37 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_37 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (37) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li37 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_37, rhs:= li_riscv_37,
   correct := by
    unfold const_llvm_37 li_riscv_37
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_38 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 38 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_38 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (38) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li38 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_38, rhs:= li_riscv_38,
   correct := by
    unfold const_llvm_38 li_riscv_38
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_39 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 39 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_39 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (39) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li39 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_39, rhs:= li_riscv_39,
   correct := by
    unfold const_llvm_39 li_riscv_39
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_40 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 40 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_40 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (40) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li40 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_40, rhs:= li_riscv_40,
   correct := by
    unfold const_llvm_40 li_riscv_40
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_41 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 41 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_41 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (41) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li41 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_41, rhs:= li_riscv_41,
   correct := by
    unfold const_llvm_41 li_riscv_41
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_42 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 42 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_42 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (42) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li42 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_42, rhs:= li_riscv_42,
   correct := by
    unfold const_llvm_42 li_riscv_42
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_43 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 43 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_43 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (43) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li43 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_43, rhs:= li_riscv_43,
   correct := by
    unfold const_llvm_43 li_riscv_43
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_44 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 44 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_44 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (44) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li44 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_44, rhs:= li_riscv_44,
   correct := by
    unfold const_llvm_44 li_riscv_44
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_45 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 45 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_45 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (45) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li45 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_45, rhs:= li_riscv_45,
   correct := by
    unfold const_llvm_45 li_riscv_45
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_46 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 46 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_46 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (46) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li46 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_46, rhs:= li_riscv_46,
   correct := by
    unfold const_llvm_46 li_riscv_46
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_47 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 47 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_47 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (47) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li47 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_47, rhs:= li_riscv_47,
   correct := by
    unfold const_llvm_47 li_riscv_47
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_48 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 48 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_48 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (48) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li48 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_48, rhs:= li_riscv_48,
   correct := by
    unfold const_llvm_48 li_riscv_48
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_49 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 49 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_49 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (49) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li49 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_49, rhs:= li_riscv_49,
   correct := by
    unfold const_llvm_49 li_riscv_49
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def li_riscv_50 := [LV| {
    ^entry ():
      %0 = "li"() {imm = 50 : !i64}  : (!i64) -> (!i64)
      %1 = "builtin.unrealized_conversion_cast"(%0) : (!i64) -> (i64)
      llvm.return %1 : i64
  }]
def const_llvm_50 : Com  LLVMPlusRiscV [] .pure (.llvm (.bitvec 64))  := [LV| {
    ^entry ():
      %1 = llvm.mlir.constant (50) : i64
      llvm.return %1 : i64
  }]
def llvm_const_lower_riscv_li50 : LLVMPeepholeRewriteRefine 64 [] :=
  {lhs:= const_llvm_50, rhs:= li_riscv_50,
   correct := by
    unfold const_llvm_50 li_riscv_50
    simp_peephole
    simp_riscv
    simp_alive_ops
    simp
  }

def all_const_llvm_const_lower_riscv_li : List (LLVMPeepholeRewriteRefine 64 []) :=
  [
  llvm_const_lower_riscv_lin50,
  llvm_const_lower_riscv_lin49,
  llvm_const_lower_riscv_lin48,
  llvm_const_lower_riscv_lin47,
  llvm_const_lower_riscv_lin46,
  llvm_const_lower_riscv_lin45,
  llvm_const_lower_riscv_lin44,
  llvm_const_lower_riscv_lin43,
  llvm_const_lower_riscv_lin42,
  llvm_const_lower_riscv_lin41,
  llvm_const_lower_riscv_lin40,
  llvm_const_lower_riscv_lin39,
  llvm_const_lower_riscv_lin38,
  llvm_const_lower_riscv_lin37,
  llvm_const_lower_riscv_lin36,
  llvm_const_lower_riscv_lin35,
  llvm_const_lower_riscv_lin34,
  llvm_const_lower_riscv_lin33,
  llvm_const_lower_riscv_lin32,
  llvm_const_lower_riscv_lin31,
  llvm_const_lower_riscv_lin30,
  llvm_const_lower_riscv_lin29,
  llvm_const_lower_riscv_lin28,
  llvm_const_lower_riscv_lin27,
  llvm_const_lower_riscv_lin26,
  llvm_const_lower_riscv_lin25,
  llvm_const_lower_riscv_lin24,
  llvm_const_lower_riscv_lin23,
  llvm_const_lower_riscv_lin22,
  llvm_const_lower_riscv_lin21,
  llvm_const_lower_riscv_lin20,
  llvm_const_lower_riscv_lin19,
  llvm_const_lower_riscv_lin18,
  llvm_const_lower_riscv_lin17,
  llvm_const_lower_riscv_lin16,
  llvm_const_lower_riscv_lin15,
  llvm_const_lower_riscv_lin14,
  llvm_const_lower_riscv_lin13,
  llvm_const_lower_riscv_lin12,
  llvm_const_lower_riscv_lin11,
  llvm_const_lower_riscv_lin10,
  llvm_const_lower_riscv_lin9,
  llvm_const_lower_riscv_lin8,
  llvm_const_lower_riscv_lin7,
  llvm_const_lower_riscv_lin6,
  llvm_const_lower_riscv_lin5,
  llvm_const_lower_riscv_lin4,
  llvm_const_lower_riscv_lin3,
  llvm_const_lower_riscv_lin2,
  llvm_const_lower_riscv_lin1,
  llvm_const_lower_riscv_li0,
  llvm_const_lower_riscv_li1,
  llvm_const_lower_riscv_li2,
  llvm_const_lower_riscv_li3,
  llvm_const_lower_riscv_li4,
  llvm_const_lower_riscv_li5,
  llvm_const_lower_riscv_li6,
  llvm_const_lower_riscv_li7,
  llvm_const_lower_riscv_li8,
  llvm_const_lower_riscv_li9,
  llvm_const_lower_riscv_li10,
  llvm_const_lower_riscv_li11,
  llvm_const_lower_riscv_li12,
  llvm_const_lower_riscv_li13,
  llvm_const_lower_riscv_li14,
  llvm_const_lower_riscv_li15,
  llvm_const_lower_riscv_li16,
  llvm_const_lower_riscv_li17,
  llvm_const_lower_riscv_li18,
  llvm_const_lower_riscv_li19,
  llvm_const_lower_riscv_li20,
  llvm_const_lower_riscv_li21,
  llvm_const_lower_riscv_li22,
  llvm_const_lower_riscv_li23,
  llvm_const_lower_riscv_li24,
  llvm_const_lower_riscv_li25,
  llvm_const_lower_riscv_li26,
  llvm_const_lower_riscv_li27,
  llvm_const_lower_riscv_li28,
  llvm_const_lower_riscv_li29,
  llvm_const_lower_riscv_li30,
  llvm_const_lower_riscv_li31,
  llvm_const_lower_riscv_li32,
  llvm_const_lower_riscv_li33,
  llvm_const_lower_riscv_li34,
  llvm_const_lower_riscv_li35,
  llvm_const_lower_riscv_li36,
  llvm_const_lower_riscv_li37,
  llvm_const_lower_riscv_li38,
  llvm_const_lower_riscv_li39,
  llvm_const_lower_riscv_li40,
  llvm_const_lower_riscv_li41,
  llvm_const_lower_riscv_li42,
  llvm_const_lower_riscv_li43,
  llvm_const_lower_riscv_li44,
  llvm_const_lower_riscv_li45,
  llvm_const_lower_riscv_li46,
  llvm_const_lower_riscv_li47,
  llvm_const_lower_riscv_li48,
  llvm_const_lower_riscv_li49,
  llvm_const_lower_riscv_li50
  ]
