-- AUTOGENERATED Lean file: Automated GlobalISel pattern translation from llvm/include/llvm/Target/GlobalISel/Combine.td
import SSA.Projects.LLVMRiscV.PeepholeRefine
import SSA.Projects.LLVMRiscV.Simpproc
import SSA.Projects.RISCV64.Tactic.SimpRiscV
import SSA.Projects.LLVMRiscV.Pipeline.mkRewrite
import SSA.Projects.LLVMRiscV.Pipeline.ConstantMatching

open LLVMRiscV

/-- ### idempotent_prop -/
def idempotent_prop : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.freeze %0 : i64
      %2 = llvm.freeze %1 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.freeze %0 : i64
      llvm.return %1 : i64
  }]

/-- ### right_identity_zero_0 -/
def right_identity_zero_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      %2 = llvm.sub %0, %1 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      llvm.return %0 : i64
  }]

/-- ### right_identity_zero_1 -/
def right_identity_zero_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      %2 = llvm.add %0, %1 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      llvm.return %0 : i64
  }]

/-- ### right_identity_zero_2 -/
def right_identity_zero_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      %2 = llvm.or %0, %1 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      llvm.return %0 : i64
  }]

/-- ### right_identity_zero_3 -/
def right_identity_zero_3 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      %2 = llvm.xor %0, %1 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      llvm.return %0 : i64
  }]

/-- ### right_identity_zero_4 -/
def right_identity_zero_4 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      %2 = llvm.shl %0, %1 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      llvm.return %0 : i64
  }]

/-- ### right_identity_zero_5 -/
def right_identity_zero_5 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      %2 = llvm.ashr %0, %1 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      llvm.return %0 : i64
  }]

/-- ### right_identity_zero_6 -/
def right_identity_zero_6 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      %2 = llvm.lshr %0, %1 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      llvm.return %0 : i64
  }]

/-- ### right_identity_one_int -/
def right_identity_one_int : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (1) : i64
      %2 = llvm.mul %0, %1 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      llvm.return %0 : i64
  }]

/-- ### binop_same_val_0 -/
def binop_same_val_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.and %0, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      llvm.return %0 : i64
  }]

/-- ### binop_same_val_1 -/
def binop_same_val_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.or %0, %0 : i64
      llvm.return %1 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      llvm.return %0 : i64
  }]

/-- ### binop_left_to_zero_0 -/
def binop_left_to_zero_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      %2 = llvm.shl %1, %0 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      llvm.return %1 : i64
  }]

/-- ### binop_left_to_zero_1 -/
def binop_left_to_zero_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      %2 = llvm.lshr %1, %0 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      llvm.return %1 : i64
  }]

/-- ### binop_left_to_zero_2 -/
def binop_left_to_zero_2 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      %2 = llvm.ashr %1, %0 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      llvm.return %1 : i64
  }]

/-- ### binop_left_to_zero_3 -/
def binop_left_to_zero_3 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      %2 = llvm.sdiv %1, %0 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      llvm.return %1 : i64
  }]

/-- ### binop_left_to_zero_4 -/
def binop_left_to_zero_4 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      %2 = llvm.udiv %1, %0 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      llvm.return %1 : i64
  }]

/-- ### binop_left_to_zero_5 -/
def binop_left_to_zero_5 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      %2 = llvm.srem %1, %0 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      llvm.return %1 : i64
  }]

/-- ### binop_left_to_zero_6 -/
def binop_left_to_zero_6 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      %2 = llvm.urem %1, %0 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      llvm.return %1 : i64
  }]

/-- ### binop_left_to_zero_7 -/
def binop_left_to_zero_7 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      %2 = llvm.mul %1, %0 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      llvm.return %1 : i64
  }]

/-- ### binop_right_to_zero -/
def binop_right_to_zero : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      %2 = llvm.mul %0, %1 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      llvm.return %1 : i64
  }]

/-- ### mul_by_neg_one -/
def mul_by_neg_one : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (-1) : i64
      %2 = llvm.mul %0, %1 : i64
      llvm.return %2 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64):
      %1 = llvm.mlir.constant (0) : i64
      %2 = llvm.sub %1, %0 : i64
      llvm.return %2 : i64
  }]

/-- ### add_sub_reg_0 -/
def add_sub_reg_0 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64), Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64, %1: i64):
      %2 = llvm.sub %0, %1 : i64
      %3 = llvm.add %1, %2 : i64
      llvm.return %3 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64, %1: i64):
      llvm.return %0 : i64
  }]

/-- ### add_sub_reg_1 -/
def add_sub_reg_1 : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64), Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64, %1: i64):
      %2 = llvm.sub %0, %1 : i64
      %3 = llvm.add %2, %1 : i64
      llvm.return %3 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64, %1: i64):
      llvm.return %0 : i64
  }]

/-- ### APlusBMinusCMinusB -/
def APlusBMinusCMinusB : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64), Ty.llvm (.bitvec 64), Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64, %1: i64, %2: i64):
      %3 = llvm.sub %1, %2 : i64
      %4 = llvm.add %0, %3 : i64
      %5 = llvm.sub %4, %1 : i64
      llvm.return %5 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64, %1: i64, %2: i64):
      %3 = llvm.sub %0, %2 : i64
      llvm.return %3 : i64
  }]

/-- ### AMinusBMinusCMinusC -/
def AMinusBMinusCMinusC : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64), Ty.llvm (.bitvec 64), Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64, %1: i64, %2: i64):
      %3 = llvm.sub %1, %2 : i64
      %4 = llvm.sub %0, %3 : i64
      %5 = llvm.sub %4, %2 : i64
      llvm.return %5 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64, %1: i64, %2: i64):
      %3 = llvm.sub %0, %1 : i64
      llvm.return %3 : i64
  }]

/-- ### ZeroMinusAPlusB -/
def ZeroMinusAPlusB : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64), Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64, %1: i64):
      %2 = llvm.mlir.constant (0) : i64
      %3 = llvm.sub %2, %0 : i64
      %4 = llvm.add %3, %1 : i64
      llvm.return %4 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64, %1: i64):
      %2 = llvm.sub %1, %0 : i64
      llvm.return %2 : i64
  }]

/-- ### APlusZeroMinusB -/
def APlusZeroMinusB : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64), Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64, %1: i64):
      %2 = llvm.mlir.constant (0) : i64
      %3 = llvm.sub %2, %1 : i64
      %4 = llvm.add %0, %3 : i64
      llvm.return %4 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64, %1: i64):
      %2 = llvm.sub %0, %1 : i64
      llvm.return %2 : i64
  }]

/-- ### APlusBMinusB -/
def APlusBMinusB : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64), Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64, %1: i64):
      %2 = llvm.sub %1, %0 : i64
      %3 = llvm.add %0, %2 : i64
      llvm.return %3 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64, %1: i64):
      llvm.return %1 : i64
  }]

/-- ### BMinusAPlusA -/
def BMinusAPlusA : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64), Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64, %1: i64):
      %2 = llvm.sub %1, %0 : i64
      %3 = llvm.add %2, %0 : i64
      llvm.return %3 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64, %1: i64):
      llvm.return %1 : i64
  }]

/-- ### AMinusBPlusCMinusA -/
def AMinusBPlusCMinusA : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64), Ty.llvm (.bitvec 64), Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64, %1: i64, %2: i64):
      %3 = llvm.sub %0, %1 : i64
      %4 = llvm.sub %2, %0 : i64
      %5 = llvm.add %3, %4 : i64
      llvm.return %5 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64, %1: i64, %2: i64):
      %3 = llvm.sub %2, %1 : i64
      llvm.return %3 : i64
  }]

/-- ### AMinusBPlusBMinusC -/
def AMinusBPlusBMinusC : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64), Ty.llvm (.bitvec 64), Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64, %1: i64, %2: i64):
      %3 = llvm.sub %0, %1 : i64
      %4 = llvm.sub %1, %2 : i64
      %5 = llvm.add %3, %4 : i64
      llvm.return %5 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64, %1: i64, %2: i64):
      %3 = llvm.sub %0, %2 : i64
      llvm.return %3 : i64
  }]

/-- ### APlusBMinusAplusC -/
def APlusBMinusAplusC : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64), Ty.llvm (.bitvec 64), Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64, %1: i64, %2: i64):
      %3 = llvm.add %0, %2 : i64
      %4 = llvm.sub %1, %3 : i64
      %5 = llvm.add %0, %4 : i64
      llvm.return %5 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64, %1: i64, %2: i64):
      %3 = llvm.sub %1, %2 : i64
      llvm.return %3 : i64
  }]

/-- ### APlusBMinusCPlusA -/
def APlusBMinusCPlusA : LLVMPeepholeRewriteRefine 64 [Ty.llvm (.bitvec 64), Ty.llvm (.bitvec 64), Ty.llvm (.bitvec 64)] where
  lhs := [LV| {
    ^entry (%0: i64, %1: i64, %2: i64):
      %3 = llvm.add %2, %0 : i64
      %4 = llvm.sub %1, %3 : i64
      %5 = llvm.add %0, %4 : i64
      llvm.return %5 : i64
  }]
  rhs := [LV| {
    ^entry (%0: i64, %1: i64, %2: i64):
      %3 = llvm.sub %1, %2 : i64
      llvm.return %3 : i64
  }]


def mir_pattern_combines : List (Σ Γ, LLVMPeepholeRewriteRefine 64 Γ) :=
  [⟨_, idempotent_prop⟩,
   ⟨_, right_identity_zero_0⟩,
   ⟨_, right_identity_zero_1⟩,
   ⟨_, right_identity_zero_2⟩,
   ⟨_, right_identity_zero_3⟩,
   ⟨_, right_identity_zero_4⟩,
   ⟨_, right_identity_zero_5⟩,
   ⟨_, right_identity_zero_6⟩,
   ⟨_, right_identity_one_int⟩,
   ⟨_, binop_same_val_0⟩,
   ⟨_, binop_same_val_1⟩,
   ⟨_, binop_left_to_zero_0⟩,
   ⟨_, binop_left_to_zero_1⟩,
   ⟨_, binop_left_to_zero_2⟩,
   ⟨_, binop_left_to_zero_3⟩,
   ⟨_, binop_left_to_zero_4⟩,
   ⟨_, binop_left_to_zero_5⟩,
   ⟨_, binop_left_to_zero_6⟩,
   ⟨_, binop_left_to_zero_7⟩,
   ⟨_, binop_right_to_zero⟩,
   ⟨_, mul_by_neg_one⟩,
   ⟨_, add_sub_reg_0⟩,
   ⟨_, add_sub_reg_1⟩,
   ⟨_, APlusBMinusCMinusB⟩,
   ⟨_, AMinusBMinusCMinusC⟩,
   ⟨_, ZeroMinusAPlusB⟩,
   ⟨_, APlusZeroMinusB⟩,
   ⟨_, APlusBMinusB⟩,
   ⟨_, BMinusAPlusA⟩,
   ⟨_, AMinusBPlusCMinusA⟩,
   ⟨_, AMinusBPlusBMinusC⟩,
   ⟨_, APlusBMinusAplusC⟩,
   ⟨_, APlusBMinusCPlusA⟩]
